{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a542d3e9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Feature engineering for electricity load forecasting\n",
    "\n",
    "The purpose of this notebook is to demonstrate how to use `skrub` and `polars`\n",
    "to perform feature engineering for electricity load forecasting.\n",
    "\n",
    "We will build a set of features from different sources:\n",
    "\n",
    "- Historical weather data for 10 medium to large urban areas in France;\n",
    "- Holidays and calendar features for France;\n",
    "- Historical electricity load data for the whole of France.\n",
    "\n",
    "All these data sources cover a time range from March 23, 2021 to May 31, 2025.\n",
    "\n",
    "Since our maximum forecasting horizon is 24 hours, we consider that the\n",
    "future weather data is known at a chosen prediction time. Similarly, the\n",
    "holidays and calendar features are known at prediction time for any point in\n",
    "the future.\n",
    "\n",
    "Therefore, features derived from the weather and calendar data can be used to\n",
    "engineer \"future covariates\". Since the load data is our prediction target,\n",
    "we will can also use it to engineer \"past covariates\" such as lagged features\n",
    "and rolling aggregations.\n",
    "\n",
    "## Environment setup\n",
    "\n",
    "We need to install some extra dependencies for this notebook if needed (when running\n",
    "jupyterlite). We need the development version of skrub to be able to use the\n",
    "skrub expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afa19ad",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "%pip install -q https://pypi.anaconda.org/ogrisel/simple/polars/1.24.0/polars-1.24.0-cp39-abi3-emscripten_3_1_58_wasm32.whl\n",
    "%pip install -q https://pypi.anaconda.org/ogrisel/simple/skrub/0.6.dev0/skrub-0.6.dev0-py3-none-any.whl\n",
    "%pip install -q altair holidays plotly nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d97b31",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# The following 3 imports are only needed to workaround some limitations\n",
    "# when using polars in a pyodide/jupyterlite notebook.\n",
    "import tzdata  # noqa: F401\n",
    "import pandas as pd\n",
    "from pyarrow.parquet import read_table\n",
    "\n",
    "import altair\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import skrub\n",
    "from pathlib import Path\n",
    "import holidays\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings from pkg_resources triggered by Python 3.13's multiprocessing.\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pkg_resources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4d1c3d",
   "metadata": {},
   "source": [
    "## Time range\n",
    "\n",
    "Let's define a hourly time range from March 23, 2021 to May 31, 2025 that\n",
    "will be used to join the electricity load data and the weather data. The time\n",
    "range is in UTC timezone to avoid any ambiguity when joining with the weather\n",
    "data that is also in UTC.\n",
    "\n",
    "We wrap the polars dataframe in a skrub variable to benefit from the\n",
    "built-in TableReport display in the notebook. Using the skrub expression\n",
    "system will also be useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4928a9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_range_start = pl.datetime(2021, 3, 23, hour=0, time_zone=\"UTC\")\n",
    "time_range_end = pl.datetime(2025, 5, 31, hour=23, time_zone=\"UTC\")\n",
    "time = skrub.var(\n",
    "    \"time\",\n",
    "    pl.DataFrame().with_columns(\n",
    "        pl.datetime_range(\n",
    "            start=time_range_start,\n",
    "            end=time_range_end,\n",
    "            time_zone=\"UTC\",\n",
    "            interval=\"1h\",\n",
    "        ).alias(\"time\"),\n",
    "    ),\n",
    ")\n",
    "time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c44f761",
   "metadata": {},
   "source": [
    "\n",
    "To avoid network issues when running this notebook, the necessary data\n",
    "files have already been downloaded and saved in the `datasets` folder.\n",
    "See the README.md file for instructions to download the data manually\n",
    "if you want to re-run this notebook with more recent data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae6c648",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_folder = Path(\"../datasets\")\n",
    "for data_file in sorted(data_source_folder.iterdir()):\n",
    "    print(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160ea36c",
   "metadata": {},
   "source": [
    "\n",
    "List of 10 medium to large urban areas to approximately cover most regions in\n",
    "France with a slight focus on most populated regions that are likely to drive\n",
    "electricity demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3aa051",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_names = [\n",
    "    \"paris\",\n",
    "    \"lyon\",\n",
    "    \"marseille\",\n",
    "    \"toulouse\",\n",
    "    \"lille\",\n",
    "    \"limoges\",\n",
    "    \"nantes\",\n",
    "    \"strasbourg\",\n",
    "    \"brest\",\n",
    "    \"bayonne\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff57920",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_city_weather_raw = {}\n",
    "for city_name in city_names:\n",
    "    # all_city_weather_raw[city_name] = skrub.var(\n",
    "    # f\"{city_name}_weather_raw\",\n",
    "    all_city_weather_raw[city_name] = (\n",
    "        pl.from_arrow(read_table(f\"../datasets/weather_{city_name}.parquet\"))\n",
    "    ).with_columns(\n",
    "        [\n",
    "            pl.col(\"time\").dt.cast_time_unit(\n",
    "                \"us\"\n",
    "            ),  # Ensure time column has the same type\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b9655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_city_weather_raw[\"brest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10926ac5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "all_city_weather_raw[\"brest\"].drop_nulls(subset=[\"temperature_2m\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f84f519",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_city_weather = time.skb.eval()\n",
    "for city_name, city_weather_raw in all_city_weather_raw.items():\n",
    "    all_city_weather = all_city_weather.join(\n",
    "        city_weather_raw.rename(\n",
    "            lambda x: x if x == \"time\" else \"weather_\" + x + \"_\" + city_name\n",
    "        ),\n",
    "        on=\"time\",\n",
    "        how=\"inner\",\n",
    "    )\n",
    "\n",
    "all_city_weather = skrub.var(\n",
    "    \"all_city_weather\",\n",
    "    all_city_weather,\n",
    ")\n",
    "all_city_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f885a5bf",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Calendar and holidays features\n",
    "\n",
    "We leverage the `holidays` package to enrich the time range with some\n",
    "calendar features such as public holidays in France. We also add some\n",
    "features that are useful for time series forecasting such as the day of the\n",
    "week, the day of the year, and the hour of the day.\n",
    "\n",
    "Note that the `holidays` package requires us to extract the date for the\n",
    "French timezone.\n",
    "\n",
    "Similarly for the calendar features: all the time features are extracted from\n",
    "the time in the French timezone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a132f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_fr = holidays.country_holidays(\"FR\", years=range(2019, 2026))\n",
    "\n",
    "fr_time = pl.col(\"time\").dt.convert_time_zone(\"Europe/Paris\")\n",
    "calendar = time.with_columns(\n",
    "    [\n",
    "        fr_time.dt.hour().alias(\"cal_hour_of_day\"),\n",
    "        fr_time.dt.weekday().alias(\"cal_day_of_week\"),\n",
    "        fr_time.dt.ordinal_day().alias(\"cal_day_of_year\"),\n",
    "        fr_time.dt.year().alias(\"cal_year\"),\n",
    "        fr_time.dt.date().is_in(holidays_fr.keys()).alias(\"cal_is_holiday\"),\n",
    "    ],\n",
    ")\n",
    "calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42d1d04",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "## Electricity load data\n",
    "\n",
    "Finally we load the electricity load data. This data will both be used as a\n",
    "target variable but also to craft some lagged and window-aggregated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef7b5b1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "load_data_files = [\n",
    "    data_file\n",
    "    for data_file in sorted(data_source_folder.iterdir())\n",
    "    if data_file.name.startswith(\"Total Load - Day Ahead\")\n",
    "    and data_file.name.endswith(\".csv\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6466cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity_raw = skrub.var(\n",
    "    \"electricity_raw\",\n",
    "    pl.concat(\n",
    "        [\n",
    "            pl.from_pandas(pd.read_csv(data_file, na_values=[\"N/A\", \"-\"])).drop(\n",
    "                [\"Day-ahead Total Load Forecast [MW] - BZN|FR\"]\n",
    "            )\n",
    "            for data_file in load_data_files\n",
    "        ],\n",
    "        how=\"vertical\",\n",
    "    ),\n",
    ")\n",
    "electricity_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f46854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity = (\n",
    "    electricity_raw.with_columns(\n",
    "        [\n",
    "            pl.col(\"Time (UTC)\")\n",
    "            .str.split(by=\" - \")\n",
    "            .list.first()\n",
    "            .str.to_datetime(\"%d.%m.%Y %H:%M\", time_zone=\"UTC\")\n",
    "            .alias(\"time\"),\n",
    "        ]\n",
    "    )\n",
    "    .drop([\"Time (UTC)\"])\n",
    "    .rename({\"Actual Total Load [MW] - BZN|FR\": \"load_mw\"})\n",
    "    .filter(pl.col(\"time\").dt.minute().eq(0))\n",
    "    .filter(pl.col(\"time\") >= time_range_start)\n",
    "    .filter(pl.col(\"time\") <= time_range_end)\n",
    "    .select([\"time\", \"load_mw\"])\n",
    ")\n",
    "electricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3492965",
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity.filter(pl.col(\"load_mw\").is_null())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f278cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "electricity.filter(\n",
    "    (pl.col(\"time\") > pl.datetime(2021, 10, 30, hour=10, time_zone=\"UTC\"))\n",
    "    & (pl.col(\"time\") < pl.datetime(2021, 10, 31, hour=10, time_zone=\"UTC\"))\n",
    ").skb.eval().plot.line(x=\"time:T\", y=\"load_mw:Q\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411c02f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity = electricity.with_columns([pl.col(\"load_mw\").interpolate()])\n",
    "electricity.filter(\n",
    "    (pl.col(\"time\") > pl.datetime(2021, 10, 30, hour=10, time_zone=\"UTC\"))\n",
    "    & (pl.col(\"time\") < pl.datetime(2021, 10, 31, hour=10, time_zone=\"UTC\"))\n",
    ").skb.eval().plot.line(x=\"time:T\", y=\"load_mw:Q\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd20ee98",
   "metadata": {},
   "source": [
    "\n",
    "Check that the number of rows matches our expectations based on\n",
    "the number of hours that separate the first and the last dates. We can do\n",
    "that by joining with the time range dataframe and checking that the number of\n",
    "rows stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298d0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    time.join(electricity, on=\"time\", how=\"inner\").shape[0] == time.shape[0]\n",
    ").skb.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd670b6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "## Lagged features\n",
    "\n",
    "We can now create some lagged features from the electricity load data.\n",
    "\n",
    "We will create 3 hourly lagged features, 1 daily lagged feature, and 1 weekly\n",
    "lagged feature. We will also create a rolling median and inter-quartile\n",
    "feature over the last 24 hours and over the last 7 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3d090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr(col, *, window_size: int):\n",
    "    \"\"\"Inter-quartile range (IQR) of a column.\"\"\"\n",
    "    return col.rolling_quantile(0.75, window_size=window_size) - col.rolling_quantile(\n",
    "        0.25, window_size=window_size\n",
    "    )\n",
    "\n",
    "\n",
    "electricity_lagged = electricity.with_columns(\n",
    "    [pl.col(\"load_mw\").shift(i).alias(f\"load_mw_lag_{i}h\") for i in range(1, 4)]\n",
    "    + [\n",
    "        pl.col(\"load_mw\").shift(24).alias(\"load_mw_lag_1d\"),\n",
    "        pl.col(\"load_mw\").shift(24 * 7).alias(\"load_mw_lag_1w\"),\n",
    "        pl.col(\"load_mw\")\n",
    "        .rolling_median(window_size=24)\n",
    "        .alias(\"load_mw_rolling_median_24h\"),\n",
    "        pl.col(\"load_mw\")\n",
    "        .rolling_median(window_size=24 * 7)\n",
    "        .alias(\"load_mw_rolling_median_7d\"),\n",
    "        iqr(pl.col(\"load_mw\"), window_size=24).alias(\"load_mw_iqr_24h\"),\n",
    "        iqr(pl.col(\"load_mw\"), window_size=24 * 7).alias(\"load_mw_iqr_7d\"),\n",
    "    ],\n",
    ")\n",
    "electricity_lagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e893bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "altair.Chart(electricity_lagged.tail(100).skb.eval()).transform_fold(\n",
    "    [\n",
    "        \"load_mw\",\n",
    "        \"load_mw_lag_1h\",\n",
    "        \"load_mw_lag_2h\",\n",
    "        \"load_mw_lag_3h\",\n",
    "        \"load_mw_lag_1d\",\n",
    "        \"load_mw_lag_1w\",\n",
    "        \"load_mw_rolling_median_24h\",\n",
    "        \"load_mw_rolling_median_7d\",\n",
    "        \"load_mw_rolling_iqr_24h\",\n",
    "        \"load_mw_rolling_iqr_7d\",\n",
    "    ],\n",
    "    as_=[\"key\", \"load_mw\"],\n",
    ").mark_line(tooltip=True).encode(x=\"time:T\", y=\"load_mw:Q\", color=\"key:N\").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03e9b74",
   "metadata": {},
   "source": [
    "## Investigating outliers in the lagged features\n",
    "\n",
    "Let's use the `skrub.TableReport` tool to look at the plots of the marginal\n",
    "distribution of the lagged features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54910d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import TableReport\n",
    "\n",
    "TableReport(electricity_lagged.skb.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26301715",
   "metadata": {},
   "source": [
    "\n",
    "Let's extract the dates where the inter-quartile range of the load is\n",
    "greater than 15,000 MW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d209b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity_lagged.filter(pl.col(\"load_mw_iqr_7d\") > 15_000)[\n",
    "    \"time\"\n",
    "].dt.date().unique().sort().to_list().skb.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3868ba8",
   "metadata": {},
   "source": [
    "\n",
    "We observe 3 date ranges with high inter-quartile range. Let's plot the\n",
    "electricity load and the lagged features for the first data range along with\n",
    "the weather data for Paris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66b75d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "altair.Chart(\n",
    "    electricity_lagged.filter(\n",
    "        (pl.col(\"time\") > pl.datetime(2021, 12, 1, time_zone=\"UTC\"))\n",
    "        & (pl.col(\"time\") < pl.datetime(2021, 12, 31, time_zone=\"UTC\"))\n",
    "    ).skb.eval()\n",
    ").transform_fold(\n",
    "    [\n",
    "        \"load_mw\",\n",
    "        \"load_mw_iqr_7d\",\n",
    "    ],\n",
    ").mark_line(\n",
    "    tooltip=True\n",
    ").encode(\n",
    "    x=\"time:T\", y=\"value:Q\", color=\"key:N\"\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67f46d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "altair.Chart(\n",
    "    all_city_weather.filter(\n",
    "        (pl.col(\"time\") > pl.datetime(2021, 12, 1, time_zone=\"UTC\"))\n",
    "        & (pl.col(\"time\") < pl.datetime(2021, 12, 31, time_zone=\"UTC\"))\n",
    "    ).skb.eval()\n",
    ").transform_fold(\n",
    "    [f\"weather_temperature_2m_{city_name}\" for city_name in city_names],\n",
    ").mark_line(\n",
    "    tooltip=True\n",
    ").encode(\n",
    "    x=\"time:T\", y=\"value:Q\", color=\"key:N\"\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36352deb",
   "metadata": {},
   "source": [
    "\n",
    "Based on the plots above, we can see that the electricity load was high just\n",
    "before the Christmas holidays due to low temperatures. Then the load suddenly\n",
    "dropped because temperatures went higher right at the start of the\n",
    "end-of-year holidays.\n",
    "\n",
    "So those outliers do not seem to be caused to a data quality issue but rather\n",
    "due to a real change in the electricity load demand. We could conduct similar\n",
    "analysis for the other date ranges with high inter-quartile range but we will\n",
    "skip that for now.\n",
    "\n",
    "If we had observed significant data quality issues over extended periods of\n",
    "time could have been addressed by removing the corresponding rows from the\n",
    "dataset. However, this would make the lagged and windowing feature\n",
    "engineering challenging to reimplement correctly. A better approach would be\n",
    "to keep a contiguous dataset assign 0 weights to the affected rows when\n",
    "fitting or evaluating the trained models via the use of the `sample_weight`\n",
    "parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91409056",
   "metadata": {},
   "source": [
    "## Final dataset\n",
    "\n",
    "We now assemble the dataset that will be used to train and evaluate the forecasting\n",
    "models via backtesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eebb837",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_time = time = skrub.var(\n",
    "    \"prediction_time\",\n",
    "    pl.DataFrame().with_columns(\n",
    "        pl.datetime_range(\n",
    "            start=time_range_start + pl.duration(days=7),\n",
    "            end=time_range_end - pl.duration(hours=24),\n",
    "            time_zone=\"UTC\",\n",
    "            interval=\"1h\",\n",
    "        ).alias(\"prediction_time\"),\n",
    "    ),\n",
    ")\n",
    "prediction_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd3c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "@skrub.deferred\n",
    "def build_features(\n",
    "    prediction_time,\n",
    "    electricity_lagged,\n",
    "    all_city_weather,\n",
    "    calendar,\n",
    "    future_feature_horizons=[1, 24],\n",
    "):\n",
    "\n",
    "    return (\n",
    "        prediction_time.join(\n",
    "            electricity_lagged, left_on=\"prediction_time\", right_on=\"time\"\n",
    "        )\n",
    "        .join(\n",
    "            all_city_weather.select(\n",
    "                [pl.col(\"time\")]\n",
    "                + [\n",
    "                    pl.col(c).shift(-h).alias(c + f\"_future_{h}h\")\n",
    "                    for c in all_city_weather.columns\n",
    "                    if c != \"time\"\n",
    "                    for h in future_feature_horizons\n",
    "                ]\n",
    "            ),\n",
    "            left_on=\"prediction_time\",\n",
    "            right_on=\"time\",\n",
    "        )\n",
    "        .join(\n",
    "            calendar.select(\n",
    "                [pl.col(\"time\")]\n",
    "                + [\n",
    "                    pl.col(c).shift(-h).alias(c + f\"_future_{h}h\")\n",
    "                    for c in calendar.columns\n",
    "                    if c != \"time\"\n",
    "                    for h in future_feature_horizons\n",
    "                ]\n",
    "            ),\n",
    "            left_on=\"prediction_time\",\n",
    "            right_on=\"time\",\n",
    "        )\n",
    "    ).drop(\"prediction_time\")\n",
    "\n",
    "\n",
    "features = build_features(\n",
    "    prediction_time=prediction_time,\n",
    "    electricity_lagged=electricity_lagged,\n",
    "    all_city_weather=all_city_weather,\n",
    "    calendar=calendar,\n",
    ").skb.mark_as_X()\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97ff81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons = range(1, 25)  # Forecasting horizons from 1 to 24 hours\n",
    "horizon_of_interest = horizons[-1]  # Focus on the 24-hour horizon\n",
    "\n",
    "target_column_name_pattern = \"load_mw_horizon_{horizon}h\"\n",
    "\n",
    "targets = prediction_time.join(\n",
    "    electricity.with_columns(\n",
    "        [\n",
    "            pl.col(\"load_mw\")\n",
    "            .shift(-h)\n",
    "            .alias(target_column_name_pattern.format(horizon=h))\n",
    "            for h in horizons\n",
    "        ]\n",
    "    ),\n",
    "    left_on=\"prediction_time\",\n",
    "    right_on=\"time\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac3b0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column_name = target_column_name_pattern.format(horizon=horizon_of_interest)\n",
    "predicted_target_column_name = \"predicted_\" + target_column_name\n",
    "target = targets[target_column_name].skb.mark_as_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cf348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import skrub.selectors as s\n",
    "\n",
    "\n",
    "predictions = features.skb.apply(\n",
    "    skrub.DropCols(\n",
    "        cols=skrub.choose_from(\n",
    "            {\n",
    "                \"none\": s.glob(\"\"),  # No column has an empty name.\n",
    "                \"load\": s.glob(\"load_*\"),\n",
    "                \"rolling_load\": s.glob(\"load_mw_rolling_*\"),\n",
    "                \"weather\": s.glob(\"weather_*\"),\n",
    "                \"temperature\": s.glob(\"weather_temperature_*\"),\n",
    "                \"moisture\": s.glob(\"weather_moisture_*\"),\n",
    "                \"cloud_cover\": s.glob(\"weather_cloud_cover_*\"),\n",
    "                \"calendar\": s.glob(\"cal_*\"),\n",
    "                \"holiday\": s.glob(\"cal_is_holiday*\"),\n",
    "                \"future_1h\": s.glob(\"*_future_1h\"),\n",
    "                \"future_24h\": s.glob(\"*_future_24h\"),\n",
    "                \"non_paris_weather\": s.glob(\"weather_*\") & ~s.glob(\"weather_*_paris_*\"),\n",
    "            },\n",
    "            name=\"dropped_cols\",\n",
    "        )\n",
    "    )\n",
    ").skb.apply(\n",
    "    HistGradientBoostingRegressor(\n",
    "        random_state=0,\n",
    "        loss=skrub.choose_from([\"squared_error\", \"poisson\", \"gamma\"], name=\"loss\"),\n",
    "        learning_rate=skrub.choose_float(\n",
    "            0.01, 1, default=0.1, log=True, name=\"learning_rate\"\n",
    "        ),\n",
    "        max_leaf_nodes=skrub.choose_int(\n",
    "            3, 300, default=30, log=True, name=\"max_leaf_nodes\"\n",
    "        ),\n",
    "    ),\n",
    "    y=target,\n",
    ")\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db65e7a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "altair.Chart(\n",
    "    pl.concat(\n",
    "        [\n",
    "            targets.skb.eval(),\n",
    "            predictions.rename(\n",
    "                {target_column_name: predicted_target_column_name}\n",
    "            ).skb.eval(),\n",
    "        ],\n",
    "        how=\"horizontal\",\n",
    "    ).tail(24 * 7)\n",
    ").transform_fold(\n",
    "    [target_column_name, predicted_target_column_name],\n",
    ").mark_line(\n",
    "    tooltip=True\n",
    ").encode(\n",
    "    x=\"prediction_time:T\", y=\"value:Q\", color=\"key:N\"\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8b0b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "max_train_size = 2 * 52 * 24 * 7  # max ~2 years of training data\n",
    "test_size = 24 * 7 * 24  # 24 weeks of test data\n",
    "gap = 7 * 24  # 1 week gap between train and test sets\n",
    "ts_cv_5 = TimeSeriesSplit(\n",
    "    n_splits=5, max_train_size=max_train_size, test_size=test_size, gap=gap\n",
    ")\n",
    "\n",
    "for cv_idx, (train_idx, test_idx) in enumerate(\n",
    "    ts_cv_5.split(prediction_time.skb.eval())\n",
    "):\n",
    "    print(f\"CV iteration #{cv_idx}\")\n",
    "    train_datetimes = prediction_time.skb.eval()[train_idx]\n",
    "    test_datetimes = prediction_time.skb.eval()[test_idx]\n",
    "    print(\n",
    "        f\"Train: {train_datetimes.shape[0]} rows, \"\n",
    "        f\"Test: {test_datetimes.shape[0]} rows\"\n",
    "    )\n",
    "    print(f\"Train time range: {train_datetimes[0, 0]} to \" f\"{train_datetimes[-1, 0]} \")\n",
    "    print(f\"Test time range: {test_datetimes[0, 0]} to \" f\"{test_datetimes[-1, 0]} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113e660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, mean_absolute_percentage_error, get_scorer\n",
    "\n",
    "\n",
    "mape_scorer = make_scorer(mean_absolute_percentage_error)\n",
    "\n",
    "cv_results = predictions.skb.cross_validate(\n",
    "    cv=ts_cv_5,\n",
    "    scoring={\n",
    "        \"r2\": get_scorer(\"r2\"),\n",
    "        \"mape\": mape_scorer,\n",
    "    },\n",
    "    return_train_score=True,\n",
    "    return_pipeline=True,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "cv_results.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6bb572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_cv_predictions(pipelines, cv_splitter, predictions, prediction_time):\n",
    "    index_generator = cv_splitter.split(prediction_time.skb.eval())\n",
    "\n",
    "    def splitter(X, y, index_generator):\n",
    "        \"\"\"Workaround to transform a scikit-learn splitter into a function understood\n",
    "        by `skrub.train_test_split`.\"\"\"\n",
    "        train_idx, test_idx = next(index_generator)\n",
    "        return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "    results = []\n",
    "    for (_, test_idx), pipeline in zip(\n",
    "        cv_splitter.split(prediction_time.skb.eval()), pipelines\n",
    "    ):\n",
    "        split = predictions.skb.train_test_split(\n",
    "            predictions.skb.get_data(),\n",
    "            splitter=splitter,\n",
    "            index_generator=index_generator,\n",
    "        )\n",
    "        results.append(\n",
    "            pl.DataFrame(\n",
    "                {\n",
    "                    \"prediction_time\": prediction_time.skb.eval()[test_idx],\n",
    "                    \"load_mw\": split[\"y_test\"],\n",
    "                    \"predicted_load_mw\": pipeline.predict(split[\"test\"]),\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec17a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_predictions = collect_cv_predictions(\n",
    "    cv_results[\"pipeline\"], ts_cv_5, predictions, prediction_time\n",
    ")\n",
    "cv_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b7615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def lorenz_curve(observed_value, predicted_value, n_samples=1_000):\n",
    "    \"\"\"Compute the Lorenz curve for a given true and predicted values.\"\"\"\n",
    "\n",
    "    def gini_index(cum_proportion_population, cum_proportion_y_true):\n",
    "        from sklearn.metrics import auc\n",
    "\n",
    "        return 1 - 2 * auc(cum_proportion_population, cum_proportion_y_true)\n",
    "\n",
    "    observed_value = np.asarray(observed_value)\n",
    "    predicted_value = np.asarray(predicted_value)\n",
    "\n",
    "    sort_idx = np.argsort(predicted_value)\n",
    "    observed_value_sorted = observed_value[sort_idx]\n",
    "\n",
    "    original_n_samples = observed_value_sorted.shape[0]\n",
    "    cum_proportion_population = np.cumsum(np.ones(original_n_samples))\n",
    "    cum_proportion_population /= cum_proportion_population[-1]\n",
    "\n",
    "    cum_proportion_y_true = np.cumsum(observed_value_sorted)\n",
    "    cum_proportion_y_true /= cum_proportion_y_true[-1]\n",
    "\n",
    "    gini_model = gini_index(cum_proportion_population, cum_proportion_y_true)\n",
    "\n",
    "    cum_proportion_population_interpolated = np.linspace(0, 1, n_samples)\n",
    "    cum_proportion_y_true_interpolated = np.interp(\n",
    "        cum_proportion_population_interpolated,\n",
    "        cum_proportion_population,\n",
    "        cum_proportion_y_true,\n",
    "    )\n",
    "\n",
    "    return pl.DataFrame(\n",
    "        {\n",
    "            \"cum_population\": cum_proportion_population_interpolated,\n",
    "            \"cum_observed\": cum_proportion_y_true_interpolated,\n",
    "        }\n",
    "    ).with_columns(\n",
    "        pl.lit(gini_model).alias(\"gini_index\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_lorenz_curve(cv_predictions, n_samples=1_000):\n",
    "    \"\"\"Plot the Lorenz curve for a given true and predicted values.\"\"\"\n",
    "\n",
    "    results = []\n",
    "    for cv_idx, predictions in enumerate(cv_predictions):\n",
    "        results.append(\n",
    "            lorenz_curve(\n",
    "                observed_value=predictions[\"load_mw\"],\n",
    "                predicted_value=predictions[\"predicted_load_mw\"],\n",
    "                n_samples=n_samples,\n",
    "            ).with_columns(\n",
    "                pl.lit(cv_idx).alias(\"cv_idx\"),\n",
    "                pl.lit(\"model\").alias(\"model\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        results.append(\n",
    "            lorenz_curve(\n",
    "                observed_value=predictions[\"load_mw\"],\n",
    "                predicted_value=predictions[\"load_mw\"],\n",
    "                n_samples=n_samples,\n",
    "            ).with_columns(\n",
    "                pl.lit(cv_idx).alias(\"cv_idx\"),\n",
    "                pl.lit(\"oracle\").alias(\"model\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    results = pl.concat(results)\n",
    "\n",
    "    gini_stats = results.group_by(\"model\").agg(\n",
    "        [\n",
    "            pl.col(\"gini_index\")\n",
    "            .mean()\n",
    "            .map_elements(lambda x: f\"{x:.4f}\", return_dtype=pl.String)\n",
    "            .alias(\"gini_mean\"),\n",
    "            pl.col(\"gini_index\")\n",
    "            .std()\n",
    "            .map_elements(lambda x: f\"{x:.4f}\", return_dtype=pl.String)\n",
    "            .alias(\"gini_std_dev\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    results = results.join(gini_stats, on=\"model\").with_columns(\n",
    "        pl.format(\"{} ({} +/- {})\", \"model\", \"gini_mean\", \"gini_std_dev\").alias(\n",
    "            \"model_label\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    diagonal_chart = (\n",
    "        altair.Chart(\n",
    "            pl.DataFrame(\n",
    "                {\n",
    "                    \"cum_population\": [0, 1],\n",
    "                    \"cum_observed\": [0, 1],\n",
    "                    \"model_label\": \"Non-informative model\",\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        .mark_line(strokeDash=[4, 4], opacity=0.5, tooltip=True)\n",
    "        .encode(\n",
    "            x=altair.X(\n",
    "                \"cum_population:Q\",\n",
    "                title=\"Fraction of observations sorted by predicted label\",\n",
    "            ),\n",
    "            y=altair.Y(\"cum_observed:Q\", title=\"Cumulative observed load proportion\"),\n",
    "            color=altair.Color(\"model_label:N\", legend=altair.Legend(title=\"Models\")),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model_chart = (\n",
    "        altair.Chart(results)\n",
    "        .mark_line(opacity=0.3, tooltip=True)\n",
    "        .encode(\n",
    "            x=altair.X(\n",
    "                \"cum_population:Q\",\n",
    "                title=\"Fraction of observations sorted by predicted label\",\n",
    "            ),\n",
    "            y=altair.Y(\"cum_observed:Q\", title=\"Cumulative observed load proportion\"),\n",
    "            color=altair.Color(\"model_label:N\", legend=altair.Legend(title=\"Models\")),\n",
    "            detail=\"cv_idx:N\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return model_chart + diagonal_chart\n",
    "\n",
    "\n",
    "plot_lorenz_curve(cv_predictions, n_samples=500).interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f90f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reliability_diagram(cv_predictions, n_bins=10):\n",
    "    # min and max load over all predictions and observations for any folds:\n",
    "    all_loads = pl.concat(\n",
    "        [\n",
    "            cv_prediction.select([\"load_mw\", \"predicted_load_mw\"])\n",
    "            for cv_prediction in cv_predictions\n",
    "        ]\n",
    "    )\n",
    "    all_loads = pl.concat(all_loads[\"load_mw\", \"predicted_load_mw\"])\n",
    "    min_load, max_load = all_loads.min(), all_loads.max()\n",
    "    scale = altair.Scale(domain=[min_load, max_load])\n",
    "\n",
    "    # Create the perfect line\n",
    "    chart = (\n",
    "        altair.Chart(\n",
    "            pl.DataFrame(\n",
    "                {\n",
    "                    \"mean_predicted_load_mw\": [min_load, max_load],\n",
    "                    \"mean_load_mw\": [min_load, max_load],\n",
    "                    \"label\": [\"Perfect\"] * 2,\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        .mark_line(tooltip=True, opacity=0.8, strokeDash=[5, 5])\n",
    "        .encode(\n",
    "            x=altair.X(\"mean_predicted_load_mw:Q\", scale=scale),\n",
    "            y=altair.Y(\"mean_load_mw:Q\", scale=scale),\n",
    "            color=altair.Color(\n",
    "                \"label:N\",\n",
    "                scale=altair.Scale(range=[\"black\"]),\n",
    "                legend=altair.Legend(title=\"Legend\"),\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add lines for each CV fold with date labels\n",
    "    for i, cv_predictions_i in enumerate(cv_predictions):\n",
    "        # Get date range for this CV fold\n",
    "        min_date = cv_predictions_i[\"prediction_time\"].min().strftime(\"%Y-%m-%d\")\n",
    "        max_date = cv_predictions_i[\"prediction_time\"].max().strftime(\"%Y-%m-%d\")\n",
    "        fold_label = f\"#{i+1} - {min_date} to {max_date}\"\n",
    "\n",
    "        mean_per_bins = (\n",
    "            cv_predictions_i.group_by(\n",
    "                pl.col(\"predicted_load_mw\").qcut(np.linspace(0, 1, n_bins))\n",
    "            )\n",
    "            .agg(\n",
    "                [\n",
    "                    pl.col(\"load_mw\").mean().alias(\"mean_load_mw\"),\n",
    "                    pl.col(\"predicted_load_mw\").mean().alias(\"mean_predicted_load_mw\"),\n",
    "                ]\n",
    "            )\n",
    "            .sort(\"predicted_load_mw\")\n",
    "            .with_columns(pl.lit(fold_label).alias(\"fold\"))\n",
    "        )\n",
    "\n",
    "        chart += (\n",
    "            altair.Chart(mean_per_bins)\n",
    "            .mark_line(tooltip=True, point=True, opacity=0.8)\n",
    "            .encode(\n",
    "                x=altair.X(\"mean_predicted_load_mw:Q\", scale=scale),\n",
    "                y=altair.Y(\"mean_load_mw:Q\", scale=scale),\n",
    "                color=altair.Color(\n",
    "                    \"fold:N\",\n",
    "                    legend=altair.Legend(title=None),\n",
    "                ),\n",
    "                detail=altair.Detail(\"fold:N\"),\n",
    "            )\n",
    "        )\n",
    "    return chart.resolve_scale(color=\"independent\")\n",
    "\n",
    "\n",
    "plot_reliability_diagram(cv_predictions).interactive().properties(\n",
    "    title=\"Reliability diagram from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed47184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals_by_hour(cv_predictions):\n",
    "    \"\"\"Plot the average residuals per hour of the day, one line and IQR band per CV fold.\"\"\"\n",
    "    all_iqr_bands = []\n",
    "    all_mean_lines = []\n",
    "\n",
    "    for i, cv_prediction in enumerate(cv_predictions):\n",
    "        # Get date range for this CV fold\n",
    "        min_date = cv_prediction[\"prediction_time\"].min().strftime(\"%Y-%m-%d\")\n",
    "        max_date = cv_prediction[\"prediction_time\"].max().strftime(\"%Y-%m-%d\")\n",
    "        fold_label = f\"#{i+1} - {min_date} to {max_date}\"\n",
    "\n",
    "        residuals_by_hour_detailed = cv_prediction.with_columns(\n",
    "            [\n",
    "                (pl.col(\"predicted_load_mw\") - pl.col(\"load_mw\")).alias(\"residual\"),\n",
    "                pl.col(\"prediction_time\").dt.hour().alias(\"hour_of_day\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Calculate statistics for this CV fold\n",
    "        residuals_stats = (\n",
    "            residuals_by_hour_detailed.group_by(\"hour_of_day\")\n",
    "            .agg(\n",
    "                [\n",
    "                    pl.col(\"residual\").mean().alias(\"mean_residual\"),\n",
    "                    pl.col(\"residual\").quantile(0.25).alias(\"q25_residual\"),\n",
    "                    pl.col(\"residual\").quantile(0.75).alias(\"q75_residual\"),\n",
    "                ]\n",
    "            )\n",
    "            .sort(\"hour_of_day\")\n",
    "            .with_columns(pl.lit(fold_label).alias(\"fold\"))\n",
    "        )\n",
    "\n",
    "        # Create IQR band for this CV fold\n",
    "        iqr_band = (\n",
    "            altair.Chart(residuals_stats)\n",
    "            .mark_area(opacity=0.15)\n",
    "            .encode(\n",
    "                x=altair.X(\"hour_of_day:O\", title=\"Hour of day\"),\n",
    "                y=altair.Y(\"q25_residual:Q\"),\n",
    "                y2=altair.Y2(\"q75_residual:Q\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Create mean line for this CV fold\n",
    "        mean_line = (\n",
    "            altair.Chart(residuals_stats)\n",
    "            .mark_line(tooltip=True, point=True, opacity=0.8)\n",
    "            .encode(\n",
    "                x=altair.X(\"hour_of_day:O\", title=\"Hour of day\"),\n",
    "                y=altair.Y(\"mean_residual:Q\", title=\"Mean residual (MW)\"),\n",
    "                color=altair.Color(\"fold:N\", legend=altair.Legend(title=\"CV Fold\")),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        all_iqr_bands.append(iqr_band)\n",
    "        all_mean_lines.append(mean_line)\n",
    "\n",
    "    # Combine all IQR bands\n",
    "    combined_iqr = all_iqr_bands[0]\n",
    "    for band in all_iqr_bands[1:]:\n",
    "        combined_iqr += band\n",
    "\n",
    "    # Combine all mean lines\n",
    "    combined_lines = all_mean_lines[0]\n",
    "    for line in all_mean_lines[1:]:\n",
    "        combined_lines += line\n",
    "\n",
    "    # Layer the IQR bands behind the mean lines\n",
    "    return (combined_iqr + combined_lines).resolve_scale(color=\"shared\")\n",
    "\n",
    "\n",
    "plot_residuals_by_hour(cv_predictions).interactive().properties(\n",
    "    title=\"Residuals by hour of the day from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e71592",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def plot_residuals_by_month(cv_predictions):\n",
    "    \"\"\"Plot the average residuals per month of the year, one line and IQR band per CV fold.\"\"\"\n",
    "    all_iqr_bands = []\n",
    "    all_mean_lines = []\n",
    "\n",
    "    for i, cv_prediction in enumerate(cv_predictions):\n",
    "        # Get date range for this CV fold\n",
    "        min_date = cv_prediction[\"prediction_time\"].min().strftime(\"%Y-%m-%d\")\n",
    "        max_date = cv_prediction[\"prediction_time\"].max().strftime(\"%Y-%m-%d\")\n",
    "        fold_label = f\"#{i+1} - {min_date} to {max_date}\"\n",
    "\n",
    "        residuals_by_month_detailed = cv_prediction.with_columns(\n",
    "            [\n",
    "                (pl.col(\"predicted_load_mw\") - pl.col(\"load_mw\")).alias(\"residual\"),\n",
    "                pl.col(\"prediction_time\").dt.month().alias(\"month_of_year\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Calculate statistics for this CV fold\n",
    "        residuals_stats = (\n",
    "            residuals_by_month_detailed.group_by(\"month_of_year\")\n",
    "            .agg(\n",
    "                [\n",
    "                    pl.col(\"residual\").mean().alias(\"mean_residual\"),\n",
    "                    pl.col(\"residual\").quantile(0.25).alias(\"q25_residual\"),\n",
    "                    pl.col(\"residual\").quantile(0.75).alias(\"q75_residual\"),\n",
    "                ]\n",
    "            )\n",
    "            .sort(\"month_of_year\")\n",
    "            .with_columns(pl.lit(fold_label).alias(\"fold\"))\n",
    "        )\n",
    "\n",
    "        # Create IQR band for this CV fold\n",
    "        iqr_band = (\n",
    "            altair.Chart(residuals_stats)\n",
    "            .mark_area(opacity=0.15)\n",
    "            .encode(\n",
    "                x=altair.X(\"month_of_year:O\", title=\"Month of year\"),\n",
    "                y=altair.Y(\"q25_residual:Q\"),\n",
    "                y2=altair.Y2(\"q75_residual:Q\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Create mean line for this CV fold\n",
    "        mean_line = (\n",
    "            altair.Chart(residuals_stats)\n",
    "            .mark_line(tooltip=True, point=True, opacity=0.8)\n",
    "            .encode(\n",
    "                x=altair.X(\"month_of_year:O\", title=\"Month of year\"),\n",
    "                y=altair.Y(\"mean_residual:Q\", title=\"Mean residual (MW)\"),\n",
    "                color=altair.Color(\"fold:N\", legend=altair.Legend(title=\"CV Fold\")),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        all_iqr_bands.append(iqr_band)\n",
    "        all_mean_lines.append(mean_line)\n",
    "\n",
    "    # Combine all IQR bands\n",
    "    combined_iqr = all_iqr_bands[0]\n",
    "    for band in all_iqr_bands[1:]:\n",
    "        combined_iqr += band\n",
    "\n",
    "    # Combine all mean lines\n",
    "    combined_lines = all_mean_lines[0]\n",
    "    for line in all_mean_lines[1:]:\n",
    "        combined_lines += line\n",
    "\n",
    "    # Layer the IQR bands behind the mean lines\n",
    "    return (combined_iqr + combined_lines).properties(\n",
    "        title=\"Residuals by month of the year from cross-validation predictions\"\n",
    "    )\n",
    "\n",
    "\n",
    "plot_residuals_by_month(cv_predictions).interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ccf19",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0342162",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "ts_cv_2 = TimeSeriesSplit(\n",
    "    n_splits=2, test_size=test_size, max_train_size=max_train_size, gap=24\n",
    ")\n",
    "randomized_search = predictions.skb.get_randomized_search(\n",
    "    cv=ts_cv_2,\n",
    "    scoring=\"r2\",\n",
    "    n_iter=100,\n",
    "    fitted=True,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45fb3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized_search.results_.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3949a37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized_search.plot_results().update_layout(margin=dict(l=150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f97ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nested_cv_results = skrub.cross_validate(\n",
    "#     environment=predictions.skb.get_data(),\n",
    "#     pipeline=randomized_search,\n",
    "#     cv=ts_cv_5,\n",
    "#     scoring={\n",
    "#         \"r2\": get_scorer(\"r2\"),\n",
    "#         \"mape\": mape_scorer,\n",
    "#     },\n",
    "#     n_jobs=-1,\n",
    "#     return_pipeline=True,\n",
    "# ).round(3)\n",
    "# nested_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221696ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for outer_cv_idx in range(len(nested_cv_results)):\n",
    "#     print(\n",
    "#         nested_cv_results.loc[outer_cv_idx, \"pipeline\"]\n",
    "#         .results_.loc[0]\n",
    "#         .round(3)\n",
    "#         .to_dict()\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc545c45",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# from joblib import Parallel, delayed\n",
    "\n",
    "# cv_predictions = []\n",
    "# for ts_cv_train_idx, ts_cv_test_idx in ts_cv_5.split(prediction_time.skb.eval()):\n",
    "#     features[ts_cv_train_idx].fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e89ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "model = MultiOutputRegressor(\n",
    "    estimator=HistGradientBoostingRegressor(random_state=0), n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79e30b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "multioutput_predictions = features.skb.apply(\n",
    "    model, y=targets.skb.drop(cols=[\"prediction_time\", \"load_mw\"]).skb.mark_as_y()\n",
    ").skb.set_name(\"multioutput_gbdt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff076fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column_names = [target_column_name_pattern.format(horizon=h) for h in horizons]\n",
    "predicted_target_column_names = [\n",
    "    f\"predicted_{target_column_name}\" for target_column_name in target_column_names\n",
    "]\n",
    "named_predictions = multioutput_predictions.rename(\n",
    "    {k: v for k, v in zip(target_column_names, predicted_target_column_names)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9146b921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "def plot_horizon_forecast(\n",
    "    targets, named_predictions, plot_at_time, historical_timedelta\n",
    "):\n",
    "    \"\"\"Plot the true target and the forecast values.\"\"\"\n",
    "    merged_data = targets.skb.select(cols=[\"prediction_time\", \"load_mw\"]).skb.concat(\n",
    "        [named_predictions], axis=1\n",
    "    )\n",
    "    start_time = plot_at_time - historical_timedelta\n",
    "    end_time = plot_at_time + datetime.timedelta(\n",
    "        hours=named_predictions.skb.eval().shape[1]\n",
    "    )\n",
    "    true_values_past = merged_data.filter(\n",
    "        pl.col(\"prediction_time\").is_between(start_time, plot_at_time, closed=\"both\")\n",
    "    ).rename({\"load_mw\": \"Past true load\"})\n",
    "    true_values_future = merged_data.filter(\n",
    "        pl.col(\"prediction_time\").is_between(plot_at_time, end_time, closed=\"both\")\n",
    "    ).rename({\"load_mw\": \"Future true load\"})\n",
    "    predicted_record = (\n",
    "        merged_data.skb.select(\n",
    "            cols=skrub.selectors.filter_names(str.startswith, \"predict\")\n",
    "        )\n",
    "        .row(by_predicate=pl.col(\"prediction_time\") == plot_at_time, named=True)\n",
    "        .skb.eval()\n",
    "    )\n",
    "    forecast_values = pl.DataFrame(\n",
    "        {\n",
    "            \"prediction_time\": predicted_record[\"prediction_time\"]\n",
    "            + datetime.timedelta(hours=horizon),\n",
    "            \"Forecast load\": predicted_record[\n",
    "                \"predicted_\" + target_column_name_pattern.format(horizon=horizon)\n",
    "            ],\n",
    "        }\n",
    "        for horizon in range(1, len(predicted_record))\n",
    "    )\n",
    "\n",
    "    true_values_past_chart = (\n",
    "        altair.Chart(true_values_past.skb.eval())\n",
    "        .transform_fold([\"Past true load\"])\n",
    "        .mark_line(tooltip=True)\n",
    "        .encode(x=\"prediction_time:T\", y=\"Past true load:Q\", color=\"key:N\")\n",
    "    )\n",
    "    true_values_future_chart = (\n",
    "        altair.Chart(true_values_future.skb.eval())\n",
    "        .transform_fold([\"Future true load\"])\n",
    "        .mark_line(tooltip=True)\n",
    "        .encode(x=\"prediction_time:T\", y=\"Future true load:Q\", color=\"key:N\")\n",
    "    )\n",
    "    forecast_values_chart = (\n",
    "        altair.Chart(forecast_values)\n",
    "        .transform_fold([\"Forecast load\"])\n",
    "        .mark_line(tooltip=True)\n",
    "        .encode(x=\"prediction_time:T\", y=\"Forecast load:Q\", color=\"key:N\")\n",
    "    )\n",
    "    return (\n",
    "        true_values_past_chart + true_values_future_chart + forecast_values_chart\n",
    "    ).interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ddfdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_at_time = datetime.datetime(2025, 5, 24, 0, 0, tzinfo=datetime.timezone.utc)\n",
    "historical_timedelta = datetime.timedelta(hours=24 * 5)\n",
    "plot_horizon_forecast(targets, named_predictions, plot_at_time, historical_timedelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e1cc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_at_time = datetime.datetime(2025, 5, 25, 0, 0, tzinfo=datetime.timezone.utc)\n",
    "plot_horizon_forecast(targets, named_predictions, plot_at_time, historical_timedelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f54167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def multioutput_scorer(regressor, X, y, score_func, score_name):\n",
    "    y_pred = regressor.predict(X)\n",
    "    return {\n",
    "        f\"{score_name}_horizon_{h}h\": score\n",
    "        for h, score in enumerate(\n",
    "            score_func(y, y_pred, multioutput=\"raw_values\"), start=1\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "def scoring(regressor, X, y):\n",
    "    return {\n",
    "        **multioutput_scorer(regressor, X, y, mean_absolute_percentage_error, \"mape\"),\n",
    "        **multioutput_scorer(regressor, X, y, r2_score, \"r2\"),\n",
    "    }\n",
    "\n",
    "\n",
    "multioutput_cv_results = multioutput_predictions.skb.cross_validate(\n",
    "    cv=ts_cv_5,\n",
    "    scoring=scoring,\n",
    "    return_train_score=True,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ").round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afe9408",
   "metadata": {},
   "outputs": [],
   "source": [
    "multioutput_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9736b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from IPython.display import display\n",
    "\n",
    "for metric_name, dataset_type in itertools.product([\"mape\", \"r2\"], [\"train\", \"test\"]):\n",
    "    columns = multioutput_cv_results.columns[\n",
    "        multioutput_cv_results.columns.str.startswith(f\"{dataset_type}_{metric_name}\")\n",
    "    ]\n",
    "    data_to_plot = multioutput_cv_results[columns]\n",
    "    data_to_plot.columns = [\n",
    "        col.replace(f\"{dataset_type}_\", \"\")\n",
    "        .replace(f\"{metric_name}_\", \"\")\n",
    "        .replace(\"_\", \" \")\n",
    "        for col in columns\n",
    "    ]\n",
    "\n",
    "    data_long = data_to_plot.melt(var_name=\"horizon\", value_name=\"score\")\n",
    "    chart = (\n",
    "        altair.Chart(\n",
    "            data_long,\n",
    "            title=f\"{dataset_type.title()} {metric_name.upper()} Scores by Horizon\",\n",
    "        )\n",
    "        .mark_boxplot(extent=\"min-max\")\n",
    "        .encode(\n",
    "            x=altair.X(\n",
    "                \"horizon:N\",\n",
    "                title=\"Horizon\",\n",
    "                sort=altair.Sort(\n",
    "                    [f\"horizon {h}h\" for h in range(1, data_to_plot.shape[1])]\n",
    "                ),\n",
    "            ),\n",
    "            y=altair.Y(\"score:Q\", title=f\"{metric_name.upper()} Score\"),\n",
    "            color=altair.Color(\"horizon:N\", legend=None),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    display(chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c174d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
