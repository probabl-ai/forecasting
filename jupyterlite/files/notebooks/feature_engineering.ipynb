{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3a27161",
   "metadata": {},
   "source": [
    "# Feature engineering for electricity load forecasting\n",
    "\n",
    "The purpose of this notebook is to demonstrate how to use `skrub` and\n",
    "`polars` to perform feature engineering for electricity load forecasting.\n",
    "\n",
    "We will build a set of features (and targets) from different data sources:\n",
    "\n",
    "- Historical weather data for 10 medium to large urban areas in France;\n",
    "- Holidays and standard calendar features for France;\n",
    "- Historical electricity load data for the whole of France.\n",
    "\n",
    "All these data sources cover a time range from March 23, 2021 to May 31,\n",
    "2025.\n",
    "\n",
    "Since our maximum forecasting horizon is 24 hours, we consider that the\n",
    "future weather data is known at a chosen prediction time. Similarly, the\n",
    "holidays and calendar features are known at prediction time for any point in\n",
    "the future.\n",
    "\n",
    "Therefore, exogenous features derived from the weather and calendar data can\n",
    "be used to engineer \"future covariates\". Since the load data is our\n",
    "prediction target, we will can also use it to engineer \"past covariates\" such\n",
    "as lagged features and rolling aggregations. The future values of the load\n",
    "data (with respect to the prediction time) are used as targets for the\n",
    "forecasting model.\n",
    "\n",
    "## Environment setup\n",
    "\n",
    "We need to install some extra dependencies for this notebook if needed (when\n",
    "running jupyterlite). We need the development version of skrub to be able to\n",
    "use the skrub expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51717eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q https://pypi.anaconda.org/ogrisel/simple/polars/1.24.0/polars-1.24.0-cp39-abi3-emscripten_3_1_58_wasm32.whl\n",
    "%pip install -q https://pypi.anaconda.org/ogrisel/simple/skrub/0.6.dev0/skrub-0.6.dev0-py3-none-any.whl\n",
    "%pip install -q altair holidays plotly nbformat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c277cc",
   "metadata": {},
   "source": [
    "\n",
    "The following 3 imports are only needed to workaround some limitations when\n",
    "using polars in a pyodide/jupyterlite notebook.\n",
    "\n",
    "TODO: remove those workarounds once pyodide 0.28 is released with support for\n",
    "the latest polars version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b919ee",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import tzdata  # noqa: F401\n",
    "import pandas as pd\n",
    "from pyarrow.parquet import read_table\n",
    "\n",
    "import altair\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import skrub\n",
    "from pathlib import Path\n",
    "import holidays\n",
    "import warnings\n",
    "\n",
    "from plotly.io import write_json, read_json  # noqa: F401\n",
    "\n",
    "from tutorial_helpers import (\n",
    "    binned_coverage,\n",
    "    plot_lorenz_curve,\n",
    "    plot_reliability_diagram,\n",
    "    plot_residuals_vs_predicted,\n",
    "    plot_binned_residuals,\n",
    "    plot_horizon_forecast,\n",
    ")\n",
    "\n",
    "# Ignore warnings from pkg_resources triggered by Python 3.13's multiprocessing.\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pkg_resources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1d3d85",
   "metadata": {},
   "source": [
    "## Shared time range for all historical data sources\n",
    "\n",
    "Let's define a hourly time range from March 23, 2021 to May 31, 2025 that\n",
    "will be used to join the electricity load data and the weather data. The time\n",
    "range is in UTC timezone to avoid any ambiguity when joining with the weather\n",
    "data that is also in UTC.\n",
    "\n",
    "We wrap the resulting polars dataframe in a `skrub` expression to benefit\n",
    "from the built-in `skrub.TableReport` display in the notebook. Using the\n",
    "`skrub` expression system will also be useful for other reasons: all\n",
    "operations in this notebook chain operations chained together in a directed\n",
    "acyclic graph that is automatically tracked by `skrub`. This allows us to\n",
    "extract the resulting pipeline and apply it to new data later on, exactly\n",
    "like a trained scikit-learn pipeline. The main difference is that we do so\n",
    "incrementally and while eagerly executing and inspecting the results of each\n",
    "step as is customary when working with dataframe libraries such as polars and\n",
    "pandas in Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8495ccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_data_start_time = skrub.var(\n",
    "    \"historical_data_start_time\", pl.datetime(2021, 3, 23, hour=0, time_zone=\"UTC\")\n",
    ")\n",
    "historical_data_end_time = skrub.var(\n",
    "    \"historical_data_end_time\", pl.datetime(2025, 5, 31, hour=23, time_zone=\"UTC\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53fd587",
   "metadata": {},
   "outputs": [],
   "source": [
    "@skrub.deferred\n",
    "def build_historical_time_range(\n",
    "    historical_data_start_time,\n",
    "    historical_data_end_time,\n",
    "    time_interval=\"1h\",\n",
    "    time_zone=\"UTC\",\n",
    "):\n",
    "    \"\"\"Define an historical time range shared by all data sources.\"\"\"\n",
    "    return pl.DataFrame().with_columns(\n",
    "        pl.datetime_range(\n",
    "            start=historical_data_start_time,\n",
    "            end=historical_data_end_time,\n",
    "            time_zone=time_zone,\n",
    "            interval=time_interval,\n",
    "        ).alias(\"time\"),\n",
    "    )\n",
    "\n",
    "\n",
    "time = build_historical_time_range(historical_data_start_time, historical_data_end_time)\n",
    "time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0e40e2",
   "metadata": {},
   "source": [
    "\n",
    "If you run the above locally with pydot and graphviz installed, you can\n",
    "visualize the expression graph of the `time` variable by expanding the \"Show\n",
    "graph\" button.\n",
    "\n",
    "Let's now load the data records for the time range defined above.\n",
    "\n",
    "To avoid network issues when running this notebook, the necessary data files\n",
    "have already been downloaded and saved in the `datasets` folder. See the\n",
    "README.md file for instructions to download the data manually if you want to\n",
    "re-run this notebook with more recent data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f4532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_folder = skrub.var(\"data_source_folder\", Path(\"../datasets\"))\n",
    "\n",
    "for data_file in sorted(data_source_folder.skb.eval().iterdir()):\n",
    "    print(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ec3e17",
   "metadata": {},
   "source": [
    "\n",
    "We define a list of 10 medium to large urban areas to approximately cover\n",
    "most regions in France with a slight focus on most populated regions that are\n",
    "likely to drive electricity demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84edabaa",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "city_names = skrub.var(\n",
    "    \"city_names\",\n",
    "    [\n",
    "        \"paris\",\n",
    "        \"lyon\",\n",
    "        \"marseille\",\n",
    "        \"toulouse\",\n",
    "        \"lille\",\n",
    "        \"limoges\",\n",
    "        \"nantes\",\n",
    "        \"strasbourg\",\n",
    "        \"brest\",\n",
    "        \"bayonne\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "@skrub.deferred\n",
    "def load_weather_data(time, city_names, data_source_folder):\n",
    "    \"\"\"Load and horizontal stack historical weather forecast data for each city.\"\"\"\n",
    "    all_city_weather = time\n",
    "    for city_name in city_names:\n",
    "        all_city_weather = all_city_weather.join(\n",
    "            pl.from_arrow(\n",
    "                read_table(f\"{data_source_folder}/weather_{city_name}.parquet\")\n",
    "            )\n",
    "            .with_columns([pl.col(\"time\").dt.cast_time_unit(\"us\")])\n",
    "            .rename(lambda x: x if x == \"time\" else \"weather_\" + x + \"_\" + city_name),\n",
    "            on=\"time\",\n",
    "        )\n",
    "    return all_city_weather\n",
    "\n",
    "\n",
    "all_city_weather = load_weather_data(time, city_names, data_source_folder)\n",
    "all_city_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7bb20c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Calendar and holidays features\n",
    "\n",
    "We leverage the `holidays` package to enrich the time range with some\n",
    "calendar features such as public holidays in France. We also add some\n",
    "features that are useful for time series forecasting such as the day of the\n",
    "week, the day of the year, and the hour of the day.\n",
    "\n",
    "Note that the `holidays` package requires us to extract the date for the\n",
    "French timezone.\n",
    "\n",
    "Similarly for the calendar features: all the time features are extracted from\n",
    "the time in the French timezone, since it is likely that electricity usage\n",
    "patterns are influenced by inhabitants' daily routines aligned with the local\n",
    "timezone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe7f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "@skrub.deferred\n",
    "def prepare_french_calendar_data(time):\n",
    "    fr_time = pl.col(\"time\").dt.convert_time_zone(\"Europe/Paris\")\n",
    "    fr_year_min = time.select(fr_time.dt.year().min()).item()\n",
    "    fr_year_max = time.select(fr_time.dt.year().max()).item()\n",
    "    holidays_fr = holidays.country_holidays(\n",
    "        \"FR\", years=range(fr_year_min, fr_year_max + 1)\n",
    "    )\n",
    "    return time.with_columns(\n",
    "        [\n",
    "            fr_time.dt.hour().alias(\"cal_hour_of_day\"),\n",
    "            fr_time.dt.weekday().alias(\"cal_day_of_week\"),\n",
    "            fr_time.dt.ordinal_day().alias(\"cal_day_of_year\"),\n",
    "            fr_time.dt.year().alias(\"cal_year\"),\n",
    "            fr_time.dt.date().is_in(holidays_fr.keys()).alias(\"cal_is_holiday\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "calendar = prepare_french_calendar_data(time)\n",
    "calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f5dcc2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "## Electricity load data\n",
    "\n",
    "Finally we load the electricity load data. This data will both be used as a\n",
    "target variable but also to craft some lagged and window-aggregated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40c1052",
   "metadata": {},
   "outputs": [],
   "source": [
    "@skrub.deferred\n",
    "def load_electricity_load_data(time, data_source_folder):\n",
    "    \"\"\"Load and aggregate historical load data from the raw CSV files.\"\"\"\n",
    "    load_data_files = [\n",
    "        data_file\n",
    "        for data_file in sorted(data_source_folder.iterdir())\n",
    "        if data_file.name.startswith(\"Total Load - Day Ahead\")\n",
    "        and data_file.name.endswith(\".csv\")\n",
    "    ]\n",
    "    return time.join(\n",
    "        (\n",
    "            pl.concat(\n",
    "                [\n",
    "                    pl.from_pandas(pd.read_csv(data_file, na_values=[\"N/A\", \"-\"])).drop(\n",
    "                        [\"Day-ahead Total Load Forecast [MW] - BZN|FR\"]\n",
    "                    )\n",
    "                    for data_file in load_data_files\n",
    "                ]\n",
    "            ).select(\n",
    "                [\n",
    "                    pl.col(\"Time (UTC)\")\n",
    "                    .str.split(by=\" - \")\n",
    "                    .list.first()\n",
    "                    .str.to_datetime(\"%d.%m.%Y %H:%M\", time_zone=\"UTC\")\n",
    "                    .alias(\"time\"),\n",
    "                    pl.col(\"Actual Total Load [MW] - BZN|FR\").alias(\"load_mw\"),\n",
    "                ]\n",
    "            )\n",
    "        ),\n",
    "        on=\"time\",\n",
    "    )\n",
    "\n",
    "\n",
    "electricity = load_electricity_load_data(time, data_source_folder)\n",
    "electricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb9660",
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity.filter(pl.col(\"load_mw\").is_null())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e55285",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "electricity.filter(\n",
    "    (pl.col(\"time\") > pl.datetime(2021, 10, 30, hour=10, time_zone=\"UTC\"))\n",
    "    & (pl.col(\"time\") < pl.datetime(2021, 10, 31, hour=10, time_zone=\"UTC\"))\n",
    ").skb.eval().plot.line(x=\"time:T\", y=\"load_mw:Q\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ff004",
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity = electricity.with_columns([pl.col(\"load_mw\").interpolate()])\n",
    "electricity.filter(\n",
    "    (pl.col(\"time\") > pl.datetime(2021, 10, 30, hour=10, time_zone=\"UTC\"))\n",
    "    & (pl.col(\"time\") < pl.datetime(2021, 10, 31, hour=10, time_zone=\"UTC\"))\n",
    ").skb.eval().plot.line(x=\"time:T\", y=\"load_mw:Q\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1588bc84",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "## Lagged features\n",
    "\n",
    "We can now create some lagged features from the electricity load data.\n",
    "\n",
    "We will create 3 hourly lagged features, 1 daily lagged feature, and 1 weekly\n",
    "lagged feature. We will also create a rolling median and inter-quartile\n",
    "feature over the last 24 hours and over the last 7 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b42cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr(col, *, window_size: int):\n",
    "    \"\"\"Inter-quartile range (IQR) of a column.\"\"\"\n",
    "    return col.rolling_quantile(0.75, window_size=window_size) - col.rolling_quantile(\n",
    "        0.25, window_size=window_size\n",
    "    )\n",
    "\n",
    "\n",
    "electricity_lagged = electricity.with_columns(\n",
    "    [pl.col(\"load_mw\").shift(i).alias(f\"load_mw_lag_{i}h\") for i in range(1, 4)]\n",
    "    + [\n",
    "        pl.col(\"load_mw\").shift(24).alias(\"load_mw_lag_1d\"),\n",
    "        pl.col(\"load_mw\").shift(24 * 7).alias(\"load_mw_lag_1w\"),\n",
    "        pl.col(\"load_mw\")\n",
    "        .rolling_median(window_size=24)\n",
    "        .alias(\"load_mw_rolling_median_24h\"),\n",
    "        pl.col(\"load_mw\")\n",
    "        .rolling_median(window_size=24 * 7)\n",
    "        .alias(\"load_mw_rolling_median_7d\"),\n",
    "        iqr(pl.col(\"load_mw\"), window_size=24).alias(\"load_mw_iqr_24h\"),\n",
    "        iqr(pl.col(\"load_mw\"), window_size=24 * 7).alias(\"load_mw_iqr_7d\"),\n",
    "    ],\n",
    ")\n",
    "electricity_lagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd617bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "altair.Chart(electricity_lagged.tail(100).skb.eval()).transform_fold(\n",
    "    [\n",
    "        \"load_mw\",\n",
    "        \"load_mw_lag_1h\",\n",
    "        \"load_mw_lag_2h\",\n",
    "        \"load_mw_lag_3h\",\n",
    "        \"load_mw_lag_1d\",\n",
    "        \"load_mw_lag_1w\",\n",
    "        \"load_mw_rolling_median_24h\",\n",
    "        \"load_mw_rolling_median_7d\",\n",
    "        \"load_mw_rolling_iqr_24h\",\n",
    "        \"load_mw_rolling_iqr_7d\",\n",
    "    ],\n",
    "    as_=[\"key\", \"load_mw\"],\n",
    ").mark_line(tooltip=True).encode(x=\"time:T\", y=\"load_mw:Q\", color=\"key:N\").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbd8562",
   "metadata": {},
   "source": [
    "\n",
    "## Important remark about lagged features engineering and system lag\n",
    "\n",
    "When working with historical data, we often have access to all the past\n",
    "measurements in the dataset. However, when we want to use the lagged features\n",
    "in a forecasting model, we need to be careful about the length of the\n",
    "**system lag**: the time between a timestamped measurement is made in the\n",
    "real world and the time the record is made available to the downstream\n",
    "application (in our case, a deployed predictive pipeline).\n",
    "\n",
    "System lag is rarely explicitly represented in the data sources even if such\n",
    "delay can be as large as several hours or even days and can sometimes be\n",
    "irregular. For instance, if there is a human intervention in the data\n",
    "recording process, holidays and weekends can punctually add significant\n",
    "delay.\n",
    "\n",
    "If the system lag is larger than the maximum feature engineering lag, the\n",
    "resulting features be filled with missing values once deployed. More\n",
    "importantly, if the system lag is not handled explicitly, those resulting\n",
    "missing values will only be present in the features computed for the\n",
    "deployed system but not present in the features computed to train and\n",
    "backtest the system before deployment.\n",
    "\n",
    "This structural discrepancy can severely degrade the performance of the\n",
    "deployed model compared to the performance estimated from backtesting on the\n",
    "historical data.\n",
    "\n",
    "We will set this problem aside for now but discuss it again in a later\n",
    "section of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730e6730",
   "metadata": {},
   "source": [
    "## Investigating outliers in the lagged features\n",
    "\n",
    "Let's use the `skrub.TableReport` tool to look at the plots of the marginal\n",
    "distribution of the lagged features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe0c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import TableReport\n",
    "\n",
    "TableReport(electricity_lagged.skb.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d767900e",
   "metadata": {},
   "source": [
    "\n",
    "Let's extract the dates where the inter-quartile range of the load is\n",
    "greater than 15,000 MW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350624c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity_lagged.filter(pl.col(\"load_mw_iqr_7d\") > 15_000)[\n",
    "    \"time\"\n",
    "].dt.date().unique().sort().to_list().skb.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa54c48",
   "metadata": {},
   "source": [
    "\n",
    "We observe 3 date ranges with high inter-quartile range. Let's plot the\n",
    "electricity load and the lagged features for the first data range along with\n",
    "the weather data for Paris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ac6baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "altair.Chart(\n",
    "    electricity_lagged.filter(\n",
    "        (pl.col(\"time\") > pl.datetime(2021, 12, 1, time_zone=\"UTC\"))\n",
    "        & (pl.col(\"time\") < pl.datetime(2021, 12, 31, time_zone=\"UTC\"))\n",
    "    ).skb.eval()\n",
    ").transform_fold(\n",
    "    [\n",
    "        \"load_mw\",\n",
    "        \"load_mw_iqr_7d\",\n",
    "    ],\n",
    ").mark_line(\n",
    "    tooltip=True\n",
    ").encode(\n",
    "    x=\"time:T\", y=\"value:Q\", color=\"key:N\"\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506ac219",
   "metadata": {},
   "outputs": [],
   "source": [
    "altair.Chart(\n",
    "    all_city_weather.filter(\n",
    "        (pl.col(\"time\") > pl.datetime(2021, 12, 1, time_zone=\"UTC\"))\n",
    "        & (pl.col(\"time\") < pl.datetime(2021, 12, 31, time_zone=\"UTC\"))\n",
    "    ).skb.eval()\n",
    ").transform_fold(\n",
    "    [f\"weather_temperature_2m_{city_name}\" for city_name in city_names.skb.eval()],\n",
    ").mark_line(\n",
    "    tooltip=True\n",
    ").encode(\n",
    "    x=\"time:T\", y=\"value:Q\", color=\"key:N\"\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87970450",
   "metadata": {},
   "source": [
    "\n",
    "Based on the plots above, we can see that the electricity load was high just\n",
    "before the Christmas holidays due to low temperatures. Then the load suddenly\n",
    "dropped because temperatures went higher right at the start of the\n",
    "end-of-year holidays.\n",
    "\n",
    "So those outliers do not seem to be caused to a data quality issue but rather\n",
    "due to a real change in the electricity load demand. We could conduct similar\n",
    "analysis for the other date ranges with high inter-quartile range but we will\n",
    "skip that for now.\n",
    "\n",
    "If we had observed significant data quality issues over extended periods of\n",
    "time could have been addressed by removing the corresponding rows from the\n",
    "dataset. However, this would make the lagged and windowing feature\n",
    "engineering challenging to reimplement correctly. A better approach would be\n",
    "to keep a contiguous dataset assign 0 weights to the affected rows when\n",
    "fitting or evaluating the trained models via the use of the `sample_weight`\n",
    "parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cda1c48",
   "metadata": {},
   "source": [
    "## Final dataset\n",
    "\n",
    "We now assemble the dataset that will be used to train and evaluate the forecasting\n",
    "models via backtesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaf0ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_start_time = skrub.var(\n",
    "    \"prediction_start_time\", historical_data_start_time.skb.eval() + pl.duration(days=7)\n",
    ")\n",
    "prediction_end_time = skrub.var(\n",
    "    \"prediction_end_time\", historical_data_end_time.skb.eval() - pl.duration(hours=24)\n",
    ")\n",
    "\n",
    "\n",
    "@skrub.deferred\n",
    "def define_prediction_time_range(prediction_start_time, prediction_end_time):\n",
    "    return pl.DataFrame().with_columns(\n",
    "        pl.datetime_range(\n",
    "            start=prediction_start_time,\n",
    "            end=prediction_end_time,\n",
    "            time_zone=\"UTC\",\n",
    "            interval=\"1h\",\n",
    "        ).alias(\"prediction_time\"),\n",
    "    )\n",
    "\n",
    "\n",
    "prediction_time = define_prediction_time_range(\n",
    "    prediction_start_time, prediction_end_time\n",
    ").skb.subsample(n=1000, how=\"head\")\n",
    "prediction_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d2eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@skrub.deferred\n",
    "def build_features(\n",
    "    prediction_time,\n",
    "    electricity_lagged,\n",
    "    all_city_weather,\n",
    "    calendar,\n",
    "    future_feature_horizons=[1, 24],\n",
    "):\n",
    "\n",
    "    return (\n",
    "        prediction_time.join(\n",
    "            electricity_lagged, left_on=\"prediction_time\", right_on=\"time\"\n",
    "        )\n",
    "        .join(\n",
    "            all_city_weather.select(\n",
    "                [pl.col(\"time\")]\n",
    "                + [\n",
    "                    pl.col(c).shift(-h).alias(c + f\"_future_{h}h\")\n",
    "                    for c in all_city_weather.columns\n",
    "                    if c != \"time\"\n",
    "                    for h in future_feature_horizons\n",
    "                ]\n",
    "            ),\n",
    "            left_on=\"prediction_time\",\n",
    "            right_on=\"time\",\n",
    "        )\n",
    "        .join(\n",
    "            calendar.select(\n",
    "                [pl.col(\"time\")]\n",
    "                + [\n",
    "                    pl.col(c).shift(-h).alias(c + f\"_future_{h}h\")\n",
    "                    for c in calendar.columns\n",
    "                    if c != \"time\"\n",
    "                    for h in future_feature_horizons\n",
    "                ]\n",
    "            ),\n",
    "            left_on=\"prediction_time\",\n",
    "            right_on=\"time\",\n",
    "        )\n",
    "    ).drop(\"prediction_time\")\n",
    "\n",
    "\n",
    "features = build_features(\n",
    "    prediction_time=prediction_time,\n",
    "    electricity_lagged=electricity_lagged,\n",
    "    all_city_weather=all_city_weather,\n",
    "    calendar=calendar,\n",
    ").skb.mark_as_X()\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb00f3e",
   "metadata": {},
   "source": [
    "\n",
    "Let's build training and evaluation targets for all possible horizons from 1\n",
    "to 24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90242d19",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "horizons = range(1, 25)\n",
    "target_column_name_pattern = \"load_mw_horizon_{horizon}h\"\n",
    "\n",
    "\n",
    "@skrub.deferred\n",
    "def build_targets(prediction_time, electricity, horizons):\n",
    "    return prediction_time.join(\n",
    "        electricity.with_columns(\n",
    "            [\n",
    "                pl.col(\"load_mw\")\n",
    "                .shift(-h)\n",
    "                .alias(target_column_name_pattern.format(horizon=h))\n",
    "                for h in horizons\n",
    "            ]\n",
    "        ),\n",
    "        left_on=\"prediction_time\",\n",
    "        right_on=\"time\",\n",
    "    )\n",
    "\n",
    "\n",
    "targets = build_targets(prediction_time, electricity, horizons)\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07df3e84",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "For now, let's focus on the last horizon (24 hours) to train a model\n",
    "predicting the electricity load at the next 24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936028e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_of_interest = horizons[-1]  # Focus on the 24-hour horizon\n",
    "target_column_name = target_column_name_pattern.format(horizon=horizon_of_interest)\n",
    "predicted_target_column_name = \"predicted_\" + target_column_name\n",
    "target = targets[target_column_name].skb.mark_as_y()\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c4cbf8",
   "metadata": {},
   "source": [
    "\n",
    "Let's define our first single output prediction pipeline. This pipeline\n",
    "chains our previous feature engineering steps with a `skrub.DropCols` step to\n",
    "drop some columns that we do not want to use as features, and a\n",
    "`HistGradientBoostingRegressor` model from scikit-learn.\n",
    "\n",
    "The `skrub.choose_from`, `skrub.choose_float`, and `skrub.choose_int`\n",
    "functions are used to define hyperparameters that can be tuned via\n",
    "cross-validated randomized search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deda3646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import skrub.selectors as s\n",
    "\n",
    "\n",
    "features_with_dropped_cols = features.skb.apply(\n",
    "    skrub.DropCols(\n",
    "        cols=skrub.choose_from(\n",
    "            {\n",
    "                \"none\": s.glob(\"\"),  # No column has an empty name.\n",
    "                \"load\": s.glob(\"load_*\"),\n",
    "                \"rolling_load\": s.glob(\"load_mw_rolling_*\"),\n",
    "                \"weather\": s.glob(\"weather_*\"),\n",
    "                \"temperature\": s.glob(\"weather_temperature_*\"),\n",
    "                \"moisture\": s.glob(\"weather_moisture_*\"),\n",
    "                \"cloud_cover\": s.glob(\"weather_cloud_cover_*\"),\n",
    "                \"calendar\": s.glob(\"cal_*\"),\n",
    "                \"holiday\": s.glob(\"cal_is_holiday*\"),\n",
    "                \"future_1h\": s.glob(\"*_future_1h\"),\n",
    "                \"future_24h\": s.glob(\"*_future_24h\"),\n",
    "                \"non_paris_weather\": s.glob(\"weather_*\") & ~s.glob(\"weather_*_paris_*\"),\n",
    "            },\n",
    "            name=\"dropped_cols\",\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "hgbr_predictions = features_with_dropped_cols.skb.apply(\n",
    "    HistGradientBoostingRegressor(\n",
    "        random_state=0,\n",
    "        loss=skrub.choose_from([\"squared_error\", \"poisson\", \"gamma\"], name=\"loss\"),\n",
    "        learning_rate=skrub.choose_float(\n",
    "            0.01, 1, default=0.1, log=True, name=\"learning_rate\"\n",
    "        ),\n",
    "        max_leaf_nodes=skrub.choose_int(\n",
    "            3, 300, default=30, log=True, name=\"max_leaf_nodes\"\n",
    "        ),\n",
    "    ),\n",
    "    y=target,\n",
    ")\n",
    "hgbr_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734357ea",
   "metadata": {},
   "source": [
    "\n",
    "The `predictions` expression captures the whole expression graph that\n",
    "includes the feature engineering steps, the target variable, and the model\n",
    "training step.\n",
    "\n",
    "In particular, the input data keys for the full pipeline can be\n",
    "inspected as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3a9ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbr_predictions.skb.get_data().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b650d8",
   "metadata": {},
   "source": [
    "\n",
    "Furthermore, the hyper-parameters of the full pipeline can be retrieved as\n",
    "follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2253cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbr_pipeline = hgbr_predictions.skb.get_pipeline()\n",
    "hgbr_pipeline.describe_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adee9e8a",
   "metadata": {},
   "source": [
    "\n",
    "When running this notebook locally, you can also interactively inspect all\n",
    "the steps of the DAG using the following (once uncommented):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8618291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.skb.full_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db35098",
   "metadata": {},
   "source": [
    "\n",
    "Since we passed input values to all the upstream `skrub` variables, `skrub`\n",
    "automatically evaluates the whole expression graph graph (train and predict\n",
    "on the same data) so that we can interactively check that everything will\n",
    "work as expected.\n",
    "\n",
    "Let's use altair to visualize the predictions against the target values for\n",
    "the last 24 hours of the prediction time range used to train the model. This\n",
    "allows us can (over)fit the data with the features at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3a309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "altair.Chart(\n",
    "    pl.concat(\n",
    "        [\n",
    "            targets.skb.preview(),\n",
    "            hgbr_predictions.rename(\n",
    "                {target_column_name: predicted_target_column_name}\n",
    "            ).skb.preview(),\n",
    "        ],\n",
    "        how=\"horizontal\",\n",
    "    ).tail(24 * 7)\n",
    ").transform_fold(\n",
    "    [target_column_name, predicted_target_column_name],\n",
    ").mark_line(\n",
    "    tooltip=True\n",
    ").encode(\n",
    "    x=\"prediction_time:T\", y=\"value:Q\", color=\"key:N\"\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b4f5b",
   "metadata": {},
   "source": [
    "\n",
    "## Assessing the model performance via cross-validation\n",
    "\n",
    "Being able to fit the training data is not enough. We need to assess the\n",
    "ability of the training pipeline to learn a predictive model that can\n",
    "generalize to unseen data.\n",
    "\n",
    "Furthermore, we want to assess the uncertainty of this estimate of the\n",
    "generalization performance via time-based cross-validation, also known as\n",
    "backtesting.\n",
    "\n",
    "scikit-learn provides a `TimeSeriesSplit` splitter providing a convenient way to\n",
    "split temporal data: in the different folds, the training data always precedes the\n",
    "test data. It implies that the size of the training data is getting larger as the\n",
    "fold index increases. The scikit-learn utility allows to define a couple of\n",
    "parameters to control the size of the training and test data and as well as a gap\n",
    "between the training and test data to potentially avoid leakage if our model relies\n",
    "on lagged features.\n",
    "\n",
    "In the example below, we define that the training data should be at most 2 years\n",
    "worth of data and the test data should be 24 weeks long. We also define a gap of\n",
    "1 week between the training.\n",
    "\n",
    "Let's check those statistics by iterating over the different folds provided by the\n",
    "splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a2bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "max_train_size = 2 * 52 * 24 * 7  # max ~2 years of training data\n",
    "test_size = 24 * 7 * 24  # 24 weeks of test data\n",
    "gap = 7 * 24  # 1 week gap between train and test sets\n",
    "ts_cv_5 = TimeSeriesSplit(\n",
    "    n_splits=5, max_train_size=max_train_size, test_size=test_size, gap=gap\n",
    ")\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(\n",
    "    ts_cv_5.split(prediction_time.skb.eval())\n",
    "):\n",
    "    print(f\"CV iteration #{fold_idx}\")\n",
    "    train_datetimes = prediction_time.skb.eval()[train_idx]\n",
    "    test_datetimes = prediction_time.skb.eval()[test_idx]\n",
    "    print(\n",
    "        f\"Train: {train_datetimes.shape[0]} rows, \"\n",
    "        f\"Test: {test_datetimes.shape[0]} rows\"\n",
    "    )\n",
    "    print(f\"Train time range: {train_datetimes[0, 0]} to \" f\"{train_datetimes[-1, 0]} \")\n",
    "    print(f\"Test time range: {test_datetimes[0, 0]} to \" f\"{test_datetimes[-1, 0]} \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f44d06a",
   "metadata": {},
   "source": [
    "\n",
    "Once the cross-validation strategy is defined, we pass it to the `cross_validate`\n",
    "function provided by `skrub` to compute the cross-validated scores. Here, we define\n",
    "the mean absolute percentage error that is interpretable. However, this metric is\n",
    "not a proper scoring rule. We therefore look at the R2 score and the Tweedie deviance\n",
    "score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fddcc7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, mean_absolute_percentage_error, get_scorer\n",
    "from sklearn.metrics import d2_tweedie_score\n",
    "\n",
    "\n",
    "hgbr_cv_results = hgbr_predictions.skb.cross_validate(\n",
    "    cv=ts_cv_5,\n",
    "    scoring={\n",
    "        \"mape\": make_scorer(mean_absolute_percentage_error),\n",
    "        \"r2\": get_scorer(\"r2\"),\n",
    "        \"d2_poisson\": make_scorer(d2_tweedie_score, power=1.0),\n",
    "        \"d2_gamma\": make_scorer(d2_tweedie_score, power=2.0),\n",
    "    },\n",
    "    return_train_score=True,\n",
    "    return_pipeline=True,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "hgbr_cv_results.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbf98aa",
   "metadata": {},
   "source": [
    "\n",
    "TODO: comment the results obtained via cross-validation.\n",
    "\n",
    "We further analyze our cross-validated model by collecting the predictions on each\n",
    "split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d292c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_cv_predictions(\n",
    "    pipelines,\n",
    "    cv_splitter,\n",
    "    predictions,\n",
    "    prediction_time,\n",
    "):\n",
    "    index_generator = cv_splitter.split(prediction_time.skb.eval())\n",
    "\n",
    "    def splitter(X, y, index_generator):\n",
    "        \"\"\"Workaround to transform a scikit-learn splitter into a function understood\n",
    "        by `skrub.train_test_split`.\"\"\"\n",
    "        train_idx, test_idx = next(index_generator)\n",
    "        return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for (_, test_idx), pipeline in zip(\n",
    "        cv_splitter.split(prediction_time.skb.eval()), pipelines\n",
    "    ):\n",
    "        split = predictions.skb.train_test_split(\n",
    "            predictions.skb.get_data(),\n",
    "            splitter=splitter,\n",
    "            index_generator=index_generator,\n",
    "        )\n",
    "        results.append(\n",
    "            pl.DataFrame(\n",
    "                {\n",
    "                    \"prediction_time\": prediction_time.skb.eval()[test_idx],\n",
    "                    \"load_mw\": split[\"y_test\"],\n",
    "                    \"predicted_load_mw\": pipeline.predict(split[\"test\"]),\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8221ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbr_cv_predictions = collect_cv_predictions(\n",
    "    hgbr_cv_results[\"pipeline\"], ts_cv_5, hgbr_predictions, prediction_time\n",
    ")\n",
    "hgbr_cv_predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4794bf",
   "metadata": {},
   "source": [
    "\n",
    "The first curve is called the Lorenz curve. It shows on the x-axis the fraction of\n",
    "observations sorted by predicted values and on the y-axis the cumulative observed\n",
    "load proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888eb9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lorenz_curve(hgbr_cv_predictions).interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ab55c7",
   "metadata": {},
   "source": [
    "\n",
    "The diagonal on the plot corresponds to a model predicting a constant value that is\n",
    "therefore not an informative model. The oracle model corresponds to the \"perfect\"\n",
    "model that would provide the an output identical to the observed values. Thus, the\n",
    "ranking of such hypothetical model is the best possible ranking. However, you should\n",
    "note that the oracle model is not the line passing through the right-hand corner of\n",
    "the plot. Instead, this curvature is defined by the distribution of the observations.\n",
    "Indeed, more the observations are composed of small values and a couple of large\n",
    "values, the more the oracle model is closer to the right-hand corner of the plot.\n",
    "\n",
    "A true model is navigating between the diagonal and the oracle model. The area between\n",
    "the diagonal and the Lorenz curve of a model is called the Gini index.\n",
    "\n",
    "For our model, we observe that each oracle model is not far from the diagonal. It\n",
    "means that the observed values do not contain a couple of large values with high\n",
    "variability. Therefore, it informs us that the complexity of our problem at hand is\n",
    "not too high. Looking at the Lorenz curve of each model, we observe that it is quite\n",
    "close to the oracle model. Therefore, the gradient boosting regressor is\n",
    "discriminative for our task.\n",
    "\n",
    "Then, we have a look at the reliability diagram. This diagram shows on the x-axis the\n",
    "mean predicted load and on the y-axis the mean observed load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04746e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_diagram(hgbr_cv_predictions).interactive().properties(\n",
    "    title=\"Reliability diagram from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515bb666",
   "metadata": {},
   "source": [
    "\n",
    "The diagonal on the reliability diagram corresponds to the best possible model: for\n",
    "a level of predicted load that fall into a bin, then the mean observed load is also\n",
    "in the same bin. If the line is above the diagonal, it means that our model is\n",
    "predicted a value too low in comparison to the observed values. If the line is below\n",
    "the diagonal, it means that our model is predicted a value too high in comparison to\n",
    "the observed values.\n",
    "\n",
    "For our cross-validated model, we observe that each reliability curve is close to the\n",
    "diagonal. We only observe a mis-calibration for the extremum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276aa017",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals_vs_predicted(hgbr_cv_predictions).interactive().properties(\n",
    "    title=\"Residuals vs Predicted Values from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f4f784",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_binned_residuals(hgbr_cv_predictions, by=\"hour\").interactive().properties(\n",
    "    title=\"Residuals by hour of the day from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5839ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_binned_residuals(hgbr_cv_predictions, by=\"month\").interactive().properties(\n",
    "    title=\"Residuals by hour of the day from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08587802",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_cv_2 = TimeSeriesSplit(\n",
    "    n_splits=2, test_size=test_size, max_train_size=max_train_size, gap=24\n",
    ")\n",
    "# randomized_search_hgbr = hgbr_predictions.skb.get_randomized_search(\n",
    "#     cv=ts_cv_2,\n",
    "#     scoring=\"r2\",\n",
    "#     n_iter=100,\n",
    "#     fitted=True,\n",
    "#     verbose=1,\n",
    "#     n_jobs=-1,\n",
    "# )\n",
    "# # %%\n",
    "# randomized_search_hgbr.results_.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0a88b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = randomized_search_hgbr.plot_results().update_layout(margin=dict(l=200))\n",
    "# write_json(fig, \"parallel_coordinates_hgbr.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98df2402",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = read_json(\"parallel_coordinates_hgbr.json\")\n",
    "fig.update_layout(margin=dict(l=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66c92cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nested_cv_results = skrub.cross_validate(\n",
    "#     environment=predictions.skb.get_data(),\n",
    "#     pipeline=randomized_search,\n",
    "#     cv=ts_cv_5,\n",
    "#     scoring={\n",
    "#         \"r2\": get_scorer(\"r2\"),\n",
    "#         \"mape\": make_scorer(mean_absolute_percentage_error),\n",
    "#     },\n",
    "#     n_jobs=-1,\n",
    "#     return_pipeline=True,\n",
    "# ).round(3)\n",
    "# nested_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1869f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for outer_fold_idx in range(len(nested_cv_results)):\n",
    "#     print(\n",
    "#         nested_cv_results.loc[outer_fold_idx, \"pipeline\"]\n",
    "#         .results_.loc[0]\n",
    "#         .round(3)\n",
    "#         .to_dict()\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b68af5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "### Exercise: non-linear feature engineering coupled with linear predictive model\n",
    "\n",
    "Now, it is your turn to make a predictive model. Towards this end, we request you\n",
    "to preprocess the input features with non-linear feature engineering:\n",
    "\n",
    "- the first step is to impute the missing values using a `SimpleImputer`. Make sure\n",
    "  to include the indicator of missing values in the feature set (i.e. look at the\n",
    "  `add_indicator` parameter);\n",
    "- use a `SplineTransformer` to create non-linear features. Use the default parameters\n",
    "  but make sure to set `sparse_output=True` since it subsequent processing will be\n",
    "  faster and more memory efficient with such data structure;\n",
    "- use a `VarianceThreshold` to remove features with potential constant features;\n",
    "- use a `SelectKBest` to select the most informative features. Set `k` to be chosen\n",
    "  from a log-uniform distribution between 100 and 1,000 (i.e. use `skrub.choose_int`);\n",
    "- use a `Nystroem` to approximate an RBF kernel. Set `n_components` to be chosen\n",
    "  from a log-uniform distribution between 10 and 200 (i.e. use `skrub.choose_int`).\n",
    "- finally, use a `Ridge` as the final predictive model. Set `alpha` to be\n",
    "  chosen from a log-uniform distribution between 1e-6 and 1e3 (i.e. use\n",
    "  `skrub.choose_float`).\n",
    "\n",
    "Use a scikit-learn `Pipeline` using `make_pipeline` to chain the steps together.\n",
    "\n",
    "Once the predictive model is defined, apply it on the `feature_with_dropped_cols`\n",
    "expression. Do not forget to define that `target` is the `y` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bce6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we provide all the imports for creating the predictive model.\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import SplineTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e886a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cab8c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ridge = features_with_dropped_cols.skb.apply(\n",
    "    make_pipeline(\n",
    "        SimpleImputer(add_indicator=True),\n",
    "        SplineTransformer(sparse_output=True),\n",
    "        VarianceThreshold(threshold=1e-6),\n",
    "        SelectKBest(\n",
    "            k=skrub.choose_int(100, 1_000, log=True, name=\"n_selected_splines\")\n",
    "        ),\n",
    "        Nystroem(\n",
    "            n_components=skrub.choose_int(\n",
    "                10, 200, log=True, name=\"n_components\", default=150\n",
    "            )\n",
    "        ),\n",
    "        Ridge(\n",
    "            alpha=skrub.choose_float(1e-6, 1e3, log=True, name=\"alpha\", default=1e-2)\n",
    "        ),\n",
    "    ),\n",
    "    y=target,\n",
    ")\n",
    "predictions_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44f1ebe",
   "metadata": {},
   "source": [
    "\n",
    "Now that you defined the predictive model, let's make a similar analysis than earlier.\n",
    "First, let's make a sanity check that plot forecast of our model on a subset of the\n",
    "training data to make a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c1e56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462d188e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "altair.Chart(\n",
    "    pl.concat(\n",
    "        [\n",
    "            targets.skb.preview(),\n",
    "            predictions_ridge.rename(\n",
    "                {target_column_name: predicted_target_column_name}\n",
    "            ).skb.preview(),\n",
    "        ],\n",
    "        how=\"horizontal\",\n",
    "    ).tail(24 * 7)\n",
    ").transform_fold(\n",
    "    [target_column_name, predicted_target_column_name],\n",
    ").mark_line(\n",
    "    tooltip=True\n",
    ").encode(\n",
    "    x=\"prediction_time:T\", y=\"value:Q\", color=\"key:N\"\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab852a1",
   "metadata": {},
   "source": [
    "\n",
    "Now, let's evaluate the performance of the model using cross-validation. Use the\n",
    "time-based cross-validation splitter `ts_cv_5` defined earlier. Make sure to compute\n",
    "the R2 score and the mean absolute percentage error. Return the training scores as\n",
    "well as the fitted pipeline such that we can make additional analysis.\n",
    "\n",
    "Does this model perform better or worse than the previous model?\n",
    "Is it underfitting or overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0263dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbc0c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_ridge = predictions_ridge.skb.cross_validate(\n",
    "    cv=ts_cv_5,\n",
    "    scoring={\n",
    "        \"r2\": get_scorer(\"r2\"),\n",
    "        \"mape\": make_scorer(mean_absolute_percentage_error),\n",
    "    },\n",
    "    return_train_score=True,\n",
    "    return_pipeline=True,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a31ff1d",
   "metadata": {},
   "source": [
    "\n",
    "Compute all cross-validated predictions to plot the Lorenz curve and the\n",
    "reliability diagram for this pipeline.\n",
    "\n",
    "To do so, you can use the function `collect_cv_predictions` to collect the\n",
    "predictions and then call the `plot_lorenz_curve` and\n",
    "`plot_reliability_diagram` functions to plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf408802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fa95ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_predictions_ridge = collect_cv_predictions(\n",
    "    cv_results_ridge[\"pipeline\"], ts_cv_5, predictions_ridge, prediction_time\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491a11e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lorenz_curve(cv_predictions_ridge).interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5fdcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_diagram(cv_predictions_ridge).interactive().properties(\n",
    "    title=\"Reliability diagram from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121aca5d",
   "metadata": {},
   "source": [
    "\n",
    "Now, let's perform a randomized search on the hyper-parameters of the model. The code\n",
    "to perform the search is shown below. Since it will be pretty computationally\n",
    "expensive, we are reloading the results of the parallel coordinates plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489af6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomized_search_ridge = predictions_ridge.skb.get_randomized_search(\n",
    "#     cv=ts_cv_2,\n",
    "#     scoring=\"r2\",\n",
    "#     n_iter=100,\n",
    "#     fitted=True,\n",
    "#     verbose=1,\n",
    "#     n_jobs=-1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44064fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = randomized_search_ridge.plot_results().update_layout(margin=dict(l=200))\n",
    "# write_json(fig, \"parallel_coordinates_ridge.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac36ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = read_json(\"parallel_coordinates_ridge.json\")\n",
    "fig.update_layout(margin=dict(l=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e6dc",
   "metadata": {},
   "source": [
    "\n",
    "We observe that the default values of the hyper-parameters are in the optimal\n",
    "region explored by the randomized search. This is a good sign that the model\n",
    "is well-specified and that the hyper-parameters are not too sensitive to\n",
    "small changes of those values.\n",
    "\n",
    "We could further assess the stability of those optimal hyper-parameters by\n",
    "running a nested cross-validation, where we would perform a randomized search\n",
    "for each fold of the outer cross-validation loop as below but this is\n",
    "computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaa7804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nested_cv_results_ridge = skrub.cross_validate(\n",
    "#     environment=predictions_ridge.skb.get_data(),\n",
    "#     pipeline=randomized_search_ridge,\n",
    "#     cv=ts_cv_5,\n",
    "#     scoring={\n",
    "#         \"r2\": get_scorer(\"r2\"),\n",
    "#         \"mape\": make_scorer(mean_absolute_percentage_error),\n",
    "#     },\n",
    "#     n_jobs=-1,\n",
    "#     return_pipeline=True,\n",
    "# ).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2403598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nested_cv_results_ridge.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1017713b",
   "metadata": {},
   "source": [
    "\n",
    "## Predicting multiple horizons with a multi-output model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de5458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "multioutput_predictions = features_with_dropped_cols.skb.apply(\n",
    "    MultiOutputRegressor(\n",
    "        estimator=HistGradientBoostingRegressor(random_state=0), n_jobs=-1\n",
    "    ),\n",
    "    y=targets.skb.drop(cols=[\"prediction_time\", \"load_mw\"]).skb.mark_as_y(),\n",
    ").skb.set_name(\"multioutput_hgbr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86122bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column_names = [target_column_name_pattern.format(horizon=h) for h in horizons]\n",
    "predicted_target_column_names = [\n",
    "    f\"predicted_{target_column_name}\" for target_column_name in target_column_names\n",
    "]\n",
    "named_predictions = multioutput_predictions.rename(\n",
    "    {k: v for k, v in zip(target_column_names, predicted_target_column_names)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b7c5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_at_time = datetime.datetime(2021, 4, 19, 0, 0, tzinfo=datetime.timezone.utc)\n",
    "historical_timedelta = datetime.timedelta(hours=24 * 5)\n",
    "plot_horizon_forecast(\n",
    "    targets,\n",
    "    named_predictions,\n",
    "    plot_at_time,\n",
    "    historical_timedelta,\n",
    "    target_column_name_pattern,\n",
    ").skb.preview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a0b273",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_at_time = datetime.datetime(2021, 4, 20, 0, 0, tzinfo=datetime.timezone.utc)\n",
    "plot_horizon_forecast(\n",
    "    targets,\n",
    "    named_predictions,\n",
    "    plot_at_time,\n",
    "    historical_timedelta,\n",
    "    target_column_name_pattern,\n",
    ").skb.preview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24164d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def multioutput_scorer(regressor, X, y, score_func, score_name):\n",
    "    y_pred = regressor.predict(X)\n",
    "    return {\n",
    "        f\"{score_name}_horizon_{h}h\": score\n",
    "        for h, score in enumerate(\n",
    "            score_func(y, y_pred, multioutput=\"raw_values\"), start=1\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "def scoring(regressor, X, y):\n",
    "    return {\n",
    "        **multioutput_scorer(regressor, X, y, mean_absolute_percentage_error, \"mape\"),\n",
    "        **multioutput_scorer(regressor, X, y, r2_score, \"r2\"),\n",
    "    }\n",
    "\n",
    "\n",
    "multioutput_cv_results = multioutput_predictions.skb.cross_validate(\n",
    "    cv=ts_cv_5,\n",
    "    scoring=scoring,\n",
    "    return_train_score=True,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ").round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa892fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "multioutput_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc31790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from IPython.display import display\n",
    "\n",
    "for metric_name, dataset_type in itertools.product([\"mape\", \"r2\"], [\"train\", \"test\"]):\n",
    "    columns = multioutput_cv_results.columns[\n",
    "        multioutput_cv_results.columns.str.startswith(f\"{dataset_type}_{metric_name}\")\n",
    "    ]\n",
    "    data_to_plot = multioutput_cv_results[columns]\n",
    "    data_to_plot.columns = [\n",
    "        col.replace(f\"{dataset_type}_\", \"\")\n",
    "        .replace(f\"{metric_name}_\", \"\")\n",
    "        .replace(\"_\", \" \")\n",
    "        for col in columns\n",
    "    ]\n",
    "\n",
    "    data_long = data_to_plot.melt(var_name=\"horizon\", value_name=\"score\")\n",
    "    chart = (\n",
    "        altair.Chart(\n",
    "            data_long,\n",
    "            title=f\"{dataset_type.title()} {metric_name.upper()} Scores by Horizon\",\n",
    "        )\n",
    "        .mark_boxplot(extent=\"min-max\")\n",
    "        .encode(\n",
    "            x=altair.X(\n",
    "                \"horizon:N\",\n",
    "                title=\"Horizon\",\n",
    "                sort=altair.Sort(\n",
    "                    [f\"horizon {h}h\" for h in range(1, data_to_plot.shape[1])]\n",
    "                ),\n",
    "            ),\n",
    "            y=altair.Y(\"score:Q\", title=f\"{metric_name.upper()} Score\"),\n",
    "            color=altair.Color(\"horizon:N\", legend=None),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    display(chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4178745c",
   "metadata": {},
   "source": [
    "\n",
    "## Native multi-output handling using `RandomForestRegressor`\n",
    "\n",
    "In the previous section, we showed how to wrap a `HistGradientBoostingRegressor`\n",
    "in a `MultiOutputRegressor` to predict multiple horizons. With such a strategy, it\n",
    "means that we trained independent `HistGradientBoostingRegressor`, one for each\n",
    "horizon.\n",
    "\n",
    "`RandomForestRegressor` natively supports multi-output regression: instead of\n",
    "independently training a model per horizon, it will train a joint model that\n",
    "predicts all horizons at once.\n",
    "\n",
    "Repeat the previous analysis using a `RandomForestRegressor`. Fix the parameter\n",
    "`min_samples_leaf` to 5.\n",
    "\n",
    "Once you created the model, plot the horizon forecast for a given date and time.\n",
    "In addition, compute the cross-validated predictions and plot the R2 and MAPE\n",
    "scores for each horizon.\n",
    "\n",
    "Does this model perform better or worse than the previous model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57622b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfc8113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30369a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "multioutput_predictions_rf = features_with_dropped_cols.skb.apply(\n",
    "    RandomForestRegressor(min_samples_leaf=5, random_state=0, n_jobs=-1),\n",
    "    y=targets.skb.drop(cols=[\"prediction_time\", \"load_mw\"]).skb.mark_as_y(),\n",
    ").skb.set_name(\"multioutput_rf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a7edb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "named_predictions_rf = multioutput_predictions_rf.rename(\n",
    "    {k: v for k, v in zip(target_column_names, predicted_target_column_names)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b70e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_at_time = datetime.datetime(2021, 4, 24, 0, 0, tzinfo=datetime.timezone.utc)\n",
    "historical_timedelta = datetime.timedelta(hours=24 * 5)\n",
    "plot_horizon_forecast(\n",
    "    targets,\n",
    "    named_predictions_rf,\n",
    "    plot_at_time,\n",
    "    historical_timedelta,\n",
    "    target_column_name_pattern,\n",
    ").skb.preview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3ed2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_at_time = datetime.datetime(2021, 4, 25, 0, 0, tzinfo=datetime.timezone.utc)\n",
    "plot_horizon_forecast(\n",
    "    targets,\n",
    "    named_predictions_rf,\n",
    "    plot_at_time,\n",
    "    historical_timedelta,\n",
    "    target_column_name_pattern,\n",
    ").skb.preview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c982f917",
   "metadata": {},
   "outputs": [],
   "source": [
    "multioutput_cv_results_rf = multioutput_predictions_rf.skb.cross_validate(\n",
    "    cv=ts_cv_5,\n",
    "    scoring=scoring,\n",
    "    return_train_score=True,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3100c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "multioutput_cv_results_rf.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a834b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from IPython.display import display\n",
    "\n",
    "for metric_name, dataset_type in itertools.product([\"mape\", \"r2\"], [\"train\", \"test\"]):\n",
    "    columns = multioutput_cv_results_rf.columns[\n",
    "        multioutput_cv_results.columns.str.startswith(f\"{dataset_type}_{metric_name}\")\n",
    "    ]\n",
    "    data_to_plot = multioutput_cv_results_rf[columns]\n",
    "    data_to_plot.columns = [\n",
    "        col.replace(f\"{dataset_type}_\", \"\")\n",
    "        .replace(f\"{metric_name}_\", \"\")\n",
    "        .replace(\"_\", \" \")\n",
    "        for col in columns\n",
    "    ]\n",
    "\n",
    "    data_long = data_to_plot.melt(var_name=\"horizon\", value_name=\"score\")\n",
    "    chart = (\n",
    "        altair.Chart(\n",
    "            data_long,\n",
    "            title=f\"{dataset_type.title()} {metric_name.upper()} Scores by Horizon\",\n",
    "        )\n",
    "        .mark_boxplot(extent=\"min-max\")\n",
    "        .encode(\n",
    "            x=altair.X(\n",
    "                \"horizon:N\",\n",
    "                title=\"Horizon\",\n",
    "                sort=altair.Sort(\n",
    "                    [f\"horizon {h}h\" for h in range(1, data_to_plot.shape[1])]\n",
    "                ),\n",
    "            ),\n",
    "            y=altair.Y(\"score:Q\", title=f\"{metric_name.upper()} Score\"),\n",
    "            color=altair.Color(\"horizon:N\", legend=None),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    display(chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb16dfd",
   "metadata": {},
   "source": [
    "\n",
    "We observe that the performance of the `RandomForestRegressor` is not better in terms\n",
    "of scores or computational cost. The trend of the scores along the horizon is also\n",
    "different from the `HistGradientBoostingRegressor`: the scores worsen as the horizon\n",
    "increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c1429e",
   "metadata": {},
   "source": [
    "\n",
    "## Uncertainty quantification using quantile regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df43d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import d2_pinball_score\n",
    "\n",
    "scoring = {\n",
    "    \"r2\": get_scorer(\"r2\"),\n",
    "    \"mape\": make_scorer(mean_absolute_percentage_error),\n",
    "    \"d2_pinball_05\": make_scorer(d2_pinball_score, alpha=0.05),\n",
    "    \"d2_pinball_50\": make_scorer(d2_pinball_score, alpha=0.50),\n",
    "    \"d2_pinball_95\": make_scorer(d2_pinball_score, alpha=0.95),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667b2385",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_params = dict(\n",
    "    loss=\"quantile\", learning_rate=0.1, max_leaf_nodes=100, random_state=0\n",
    ")\n",
    "predictions_hgbr_05 = features_with_dropped_cols.skb.apply(\n",
    "    HistGradientBoostingRegressor(**common_params, quantile=0.05),\n",
    "    y=target,\n",
    ")\n",
    "predictions_hgbr_50 = features_with_dropped_cols.skb.apply(\n",
    "    HistGradientBoostingRegressor(**common_params, quantile=0.5),\n",
    "    y=target,\n",
    ")\n",
    "predictions_hgbr_95 = features_with_dropped_cols.skb.apply(\n",
    "    HistGradientBoostingRegressor(**common_params, quantile=0.95),\n",
    "    y=target,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb543ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_hgbr_05 = predictions_hgbr_05.skb.cross_validate(\n",
    "    cv=ts_cv_5,\n",
    "    scoring=scoring,\n",
    "    return_pipeline=True,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "cv_results_hgbr_50 = predictions_hgbr_50.skb.cross_validate(\n",
    "    cv=ts_cv_5,\n",
    "    scoring=scoring,\n",
    "    return_pipeline=True,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "cv_results_hgbr_95 = predictions_hgbr_95.skb.cross_validate(\n",
    "    cv=ts_cv_5,\n",
    "    scoring=scoring,\n",
    "    return_pipeline=True,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fb7023",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_hgbr_05[\n",
    "    [col for col in cv_results_hgbr_05.columns if col.startswith(\"test_\")]\n",
    "].mean(axis=0).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342bf873",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_hgbr_50[\n",
    "    [col for col in cv_results_hgbr_50.columns if col.startswith(\"test_\")]\n",
    "].mean(axis=0).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bca888",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_hgbr_95[\n",
    "    [col for col in cv_results_hgbr_95.columns if col.startswith(\"test_\")]\n",
    "].mean(axis=0).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987b2330",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pl.concat(\n",
    "    [\n",
    "        targets.skb.select(cols=[\"prediction_time\", target_column_name]).skb.eval(),\n",
    "        predictions_hgbr_05.rename({target_column_name: \"quantile_05\"}).skb.eval(),\n",
    "        predictions_hgbr_50.rename({target_column_name: \"median\"}).skb.eval(),\n",
    "        predictions_hgbr_95.rename({target_column_name: \"quantile_95\"}).skb.eval(),\n",
    "    ],\n",
    "    how=\"horizontal\",\n",
    ").tail(24 * 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee059f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_chart = (\n",
    "    altair.Chart(results)\n",
    "    .transform_fold([target_column_name, \"median\"])\n",
    "    .mark_line(tooltip=True)\n",
    "    .encode(x=\"prediction_time:T\", y=\"value:Q\", color=\"key:N\")\n",
    ")\n",
    "\n",
    "quantile_band_chart = (\n",
    "    altair.Chart(results)\n",
    "    .mark_area(opacity=0.4, tooltip=True)\n",
    "    .encode(\n",
    "        x=\"prediction_time:T\",\n",
    "        y=\"quantile_05:Q\",\n",
    "        y2=\"quantile_95:Q\",\n",
    "        color=altair.value(\"lightgreen\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "combined_chart = quantile_band_chart + median_chart\n",
    "combined_chart.interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af76824e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_predictions_hgbr_05 = collect_cv_predictions(\n",
    "    cv_results_hgbr_05[\"pipeline\"], ts_cv_5, predictions_hgbr_05, prediction_time\n",
    ")\n",
    "cv_predictions_hgbr_50 = collect_cv_predictions(\n",
    "    cv_results_hgbr_50[\"pipeline\"], ts_cv_5, predictions_hgbr_50, prediction_time\n",
    ")\n",
    "cv_predictions_hgbr_95 = collect_cv_predictions(\n",
    "    cv_results_hgbr_95[\"pipeline\"], ts_cv_5, predictions_hgbr_95, prediction_time\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be800383",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals_vs_predicted(cv_predictions_hgbr_05).interactive().properties(\n",
    "    title=(\n",
    "        \"Residuals vs Predicted Values from cross-validation predictions\"\n",
    "        \" for quantile 0.05\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba8374",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals_vs_predicted(cv_predictions_hgbr_50).interactive().properties(\n",
    "    title=(\n",
    "        \"Residuals vs Predicted Values from cross-validation predictions\" \" for median\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b75f8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals_vs_predicted(cv_predictions_hgbr_95).interactive().properties(\n",
    "    title=(\n",
    "        \"Residuals vs Predicted Values from cross-validation predictions\"\n",
    "        \" for quantile 0.95\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a389fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_predictions_hgbr_05_concat = pl.concat(cv_predictions_hgbr_05, how=\"vertical\")\n",
    "cv_predictions_hgbr_50_concat = pl.concat(cv_predictions_hgbr_50, how=\"vertical\")\n",
    "cv_predictions_hgbr_95_concat = pl.concat(cv_predictions_hgbr_95, how=\"vertical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7525f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "\n",
    "\n",
    "for kind in [\"actual_vs_predicted\", \"residual_vs_predicted\"]:\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "\n",
    "    PredictionErrorDisplay.from_predictions(\n",
    "        y_true=cv_predictions_hgbr_05_concat[\"load_mw\"].to_numpy(),\n",
    "        y_pred=cv_predictions_hgbr_05_concat[\"predicted_load_mw\"].to_numpy(),\n",
    "        kind=kind,\n",
    "        ax=axs[0],\n",
    "    )\n",
    "    axs[0].set_title(\"0.05 quantile regression\")\n",
    "\n",
    "    PredictionErrorDisplay.from_predictions(\n",
    "        y_true=cv_predictions_hgbr_50_concat[\"load_mw\"].to_numpy(),\n",
    "        y_pred=cv_predictions_hgbr_50_concat[\"predicted_load_mw\"].to_numpy(),\n",
    "        kind=kind,\n",
    "        ax=axs[1],\n",
    "    )\n",
    "    axs[1].set_title(\"Median regression\")\n",
    "\n",
    "    PredictionErrorDisplay.from_predictions(\n",
    "        y_true=cv_predictions_hgbr_95_concat[\"load_mw\"].to_numpy(),\n",
    "        y_pred=cv_predictions_hgbr_95_concat[\"predicted_load_mw\"].to_numpy(),\n",
    "        kind=kind,\n",
    "        ax=axs[2],\n",
    "    )\n",
    "    axs[2].set_title(\"0.95 quantile regression\")\n",
    "\n",
    "    fig.suptitle(f\"{kind} for GBRT minimzing different quantile losses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52439336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage(y_true, y_quantile_low, y_quantile_high):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_quantile_low = np.asarray(y_quantile_low)\n",
    "    y_quantile_high = np.asarray(y_quantile_high)\n",
    "    return float(\n",
    "        np.logical_and(y_true >= y_quantile_low, y_true <= y_quantile_high)\n",
    "        .mean()\n",
    "        .round(4)\n",
    "    )\n",
    "\n",
    "\n",
    "def mean_width(y_true, y_quantile_low, y_quantile_high):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_quantile_low = np.asarray(y_quantile_low)\n",
    "    y_quantile_high = np.asarray(y_quantile_high)\n",
    "    return float(np.abs(y_quantile_high - y_quantile_low).mean().round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f092049",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage(\n",
    "    cv_predictions_hgbr_50_concat[\"load_mw\"].to_numpy(),\n",
    "    cv_predictions_hgbr_05_concat[\"predicted_load_mw\"].to_numpy(),\n",
    "    cv_predictions_hgbr_95_concat[\"predicted_load_mw\"].to_numpy(),\n",
    ")\n",
    "\n",
    "mean_width(\n",
    "    cv_predictions_hgbr_50_concat[\"load_mw\"].to_numpy(),\n",
    "    cv_predictions_hgbr_05_concat[\"predicted_load_mw\"].to_numpy(),\n",
    "    cv_predictions_hgbr_95_concat[\"predicted_load_mw\"].to_numpy(),\n",
    ")\n",
    "\n",
    "# Compute binned coverage scores\n",
    "binned_coverage_results = binned_coverage(\n",
    "    [df[\"load_mw\"].to_numpy() for df in cv_predictions_hgbr_50],\n",
    "    [df[\"predicted_load_mw\"].to_numpy() for df in cv_predictions_hgbr_05],\n",
    "    [df[\"predicted_load_mw\"].to_numpy() for df in cv_predictions_hgbr_95],\n",
    "    n_bins=10,\n",
    ")\n",
    "\n",
    "binned_coverage_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6a0e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_by_bin = binned_coverage_results.copy()\n",
    "coverage_by_bin[\"bin_label\"] = coverage_by_bin.apply(\n",
    "    lambda row: f\"[{row.bin_left:.0f}, {row.bin_right:.0f}]\", axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b70b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = coverage_by_bin.boxplot(\n",
    "    column=\"coverage\", by=\"bin_label\", figsize=(12, 6), vert=False, whis=1000\n",
    ")\n",
    "ax.axvline(x=0.9, color=\"red\", linestyle=\"--\", label=\"Target coverage (0.9)\")\n",
    "ax.set_xlabel(\"Load bins (MW)\")\n",
    "ax.set_ylabel(\"Coverage\")\n",
    "ax.set_title(\"Coverage Distribution by Load Bins\")\n",
    "ax.legend()\n",
    "plt.suptitle(\"\")  # Remove automatic suptitle from boxplot\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cfeada",
   "metadata": {},
   "source": [
    "\n",
    "## Reliability diagrams and Lorenz curves for quantile regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37380da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_diagram(\n",
    "    cv_predictions_hgbr_50, kind=\"quantile\", quantile_level=0.50\n",
    ").interactive().properties(\n",
    "    title=\"Reliability diagram for quantile 0.50 from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee89e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_diagram(\n",
    "    cv_predictions_hgbr_05, kind=\"quantile\", quantile_level=0.05\n",
    ").interactive().properties(\n",
    "    title=\"Reliability diagram for quantile 0.05 from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1961e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_diagram(\n",
    "    cv_predictions_hgbr_95, kind=\"quantile\", quantile_level=0.95\n",
    ").interactive().properties(\n",
    "    title=\"Reliability diagram for quantile 0.95 from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0f8f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lorenz_curve(cv_predictions_hgbr_50).interactive().properties(\n",
    "    title=\"Lorenz curve for quantile 0.50 from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4822a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lorenz_curve(cv_predictions_hgbr_05).interactive().properties(\n",
    "    title=\"Lorenz curve for quantile 0.05 from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422acf26",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plot_lorenz_curve(cv_predictions_hgbr_95).interactive().properties(\n",
    "    title=\"Lorenz curve for quantile 0.95 from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ea0938",
   "metadata": {},
   "source": [
    "\n",
    "## Quantile regression as classification\n",
    "\n",
    "In the following, we turn a quantile regression problem for all possible\n",
    "quantile levels into a multiclass classification problem by discretizing the\n",
    "target variable into bins and interpolating the cumulative sum of the bin\n",
    "membership probability to estimate the CDF of the distribution of the\n",
    "continuous target variable conditioned on the features.\n",
    "\n",
    "Ideally, the classifier should be efficient when trained on a large number of\n",
    "classes (induced by the number of bins). Therefore we use a Random Forest\n",
    "classifier as the default base estimator.\n",
    "\n",
    "There are several advantages to this approach:\n",
    "- a single model is trained and can jointly estimate quantiles for all\n",
    "  quantile levels (assuming a well tuned number of bins);\n",
    "- the quantile levels can be chosen at prediction time, which allows for a\n",
    "  flexible quantile regression model;\n",
    "- in practice, the resulting predictions are often reasonably well calibrated\n",
    "  as we will see in the reliability diagrams below.\n",
    "\n",
    "One possible drawback is that current implementations of gradient boosting\n",
    "models tend to be very slow to train with a large number of classes. Random\n",
    "Forests are much more efficient in this case, but they do not always provide\n",
    "the best predictive performance. It could be the case that combining this\n",
    "approach with tabular neural networks can lead to competitive results.\n",
    "\n",
    "However, the current scikit-learn API is not expressive enough to to handle\n",
    "the output shape of the quantile prediction function. We therefore cannot\n",
    "make it fit into a skrub pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a979ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.utils.validation import check_consistent_length\n",
    "from sklearn.utils import check_random_state\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BinnedQuantileRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimator=None,\n",
    "        n_bins=100,\n",
    "        quantile=0.5,\n",
    "        random_state=None,\n",
    "    ):\n",
    "        self.n_bins = n_bins\n",
    "        self.estimator = estimator\n",
    "        self.quantile = quantile\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Lightweight input validation: most of the input validation will be\n",
    "        # handled by the sub estimators.\n",
    "        random_state = check_random_state(self.random_state)\n",
    "        check_consistent_length(X, y)\n",
    "        self.target_binner_ = KBinsDiscretizer(\n",
    "            n_bins=self.n_bins,\n",
    "            strategy=\"quantile\",\n",
    "            subsample=200_000,\n",
    "            encode=\"ordinal\",\n",
    "            quantile_method=\"averaged_inverted_cdf\",\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "        y_binned = (\n",
    "            self.target_binner_.fit_transform(np.asarray(y).reshape(-1, 1))\n",
    "            .ravel()\n",
    "            .astype(np.int32)\n",
    "        )\n",
    "\n",
    "        # Fit the multiclass classifier to predict the binned targets from the\n",
    "        # training set.\n",
    "        if self.estimator is None:\n",
    "            estimator = RandomForestClassifier(random_state=random_state)\n",
    "        else:\n",
    "            estimator = clone(self.estimator)\n",
    "        self.estimator_ = estimator.fit(X, y_binned)\n",
    "        return self\n",
    "\n",
    "    def predict_quantiles(self, X, quantiles=(0.05, 0.5, 0.95)):\n",
    "        check_is_fitted(self, \"estimator_\")\n",
    "        edges = self.target_binner_.bin_edges_[0]\n",
    "        n_bins = edges.shape[0] - 1\n",
    "        expected_shape = (X.shape[0], n_bins)\n",
    "        y_proba_raw = self.estimator_.predict_proba(X)\n",
    "\n",
    "        # Some might stay empty on the training set. Typically, classifiers do\n",
    "        # not learn to predict an explicit 0 probability for unobserved classes\n",
    "        # so we have to post process their output:\n",
    "        if y_proba_raw.shape != expected_shape:\n",
    "            y_proba = np.zeros(shape=expected_shape)\n",
    "            y_proba[:, self.estimator_.classes_] = y_proba_raw\n",
    "        else:\n",
    "            y_proba = y_proba_raw\n",
    "\n",
    "        # Build the mapper for inverse CDF mapping, from cumulated\n",
    "        # probabilities to continuous prediction.\n",
    "        y_cdf = np.zeros(shape=(X.shape[0], edges.shape[0]))\n",
    "        y_cdf[:, 1:] = np.cumsum(y_proba, axis=1)\n",
    "        return np.asarray([interp1d(y_cdf_i, edges)(quantiles) for y_cdf_i in y_cdf])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.predict_quantiles(X, quantiles=(self.quantile,)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abcb9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = (0.05, 0.5, 0.95)\n",
    "bqr = BinnedQuantileRegressor(\n",
    "    RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        min_samples_leaf=5,\n",
    "        max_features=0.2,\n",
    "        n_jobs=-1,\n",
    "        random_state=0,\n",
    "    ),\n",
    "    n_bins=30,\n",
    ")\n",
    "bqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44ac4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "X, y = features_with_dropped_cols.skb.eval(), target.skb.eval()\n",
    "\n",
    "cv_results_bqr = cross_validate(\n",
    "    bqr,\n",
    "    X,\n",
    "    y,\n",
    "    cv=ts_cv_5,\n",
    "    scoring={\n",
    "        \"d2_pinball_50\": make_scorer(d2_pinball_score, alpha=0.5),\n",
    "    },\n",
    "    return_estimator=True,\n",
    "    return_indices=True,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae5f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_predictions_bqr_all = [\n",
    "    cv_predictions_bqr_05 := [],\n",
    "    cv_predictions_bqr_50 := [],\n",
    "    cv_predictions_bqr_95 := [],\n",
    "]\n",
    "for fold_ix, (qreg, test_idx) in enumerate(\n",
    "    zip(cv_results_bqr[\"estimator\"], cv_results_bqr[\"indices\"][\"test\"])\n",
    "):\n",
    "    print(f\"CV iteration #{fold_ix}\")\n",
    "    print(f\"Test set size: {test_idx.shape[0]} rows\")\n",
    "    print(\n",
    "        f\"Test time range: {prediction_time.skb.eval()[test_idx][0, 0]} to \"\n",
    "        f\"{prediction_time.skb.eval()[test_idx][-1, 0]} \"\n",
    "    )\n",
    "    y_pred_all_quantiles = qreg.predict_quantiles(X[test_idx], quantiles=quantiles)\n",
    "\n",
    "    coverage_score = coverage(\n",
    "        y[test_idx],\n",
    "        y_pred_all_quantiles[:, 0],\n",
    "        y_pred_all_quantiles[:, 2],\n",
    "    )\n",
    "    print(f\"Coverage: {coverage_score:.3f}\")\n",
    "\n",
    "    mean_width_score = mean_width(\n",
    "        y[test_idx],\n",
    "        y_pred_all_quantiles[:, 0],\n",
    "        y_pred_all_quantiles[:, 2],\n",
    "    )\n",
    "    print(f\"Mean prediction interval width: \" f\"{mean_width_score:.1f} MW\")\n",
    "\n",
    "    for q_idx, (quantile, predictions) in enumerate(\n",
    "        zip(quantiles, cv_predictions_bqr_all)\n",
    "    ):\n",
    "        observed = y[test_idx]\n",
    "        predicted = y_pred_all_quantiles[:, q_idx]\n",
    "        predictions.append(\n",
    "            pl.DataFrame(\n",
    "                {\n",
    "                    \"prediction_time\": prediction_time.skb.eval()[test_idx],\n",
    "                    \"load_mw\": observed,\n",
    "                    \"predicted_load_mw\": predicted,\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        print(f\"d2_pinball score: {d2_pinball_score(observed, predicted):.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f323de3",
   "metadata": {
    "title": "[markdown"
   },
   "outputs": [],
   "source": [
    "# Let's assess the calibration of the quantile regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200c591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_diagram(\n",
    "    cv_predictions_bqr_50, kind=\"quantile\", quantile_level=0.50\n",
    ").interactive().properties(\n",
    "    title=\"Reliability diagram for quantile 0.50 from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b20f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_diagram(\n",
    "    cv_predictions_bqr_05, kind=\"quantile\", quantile_level=0.05\n",
    ").interactive().properties(\n",
    "    title=\"Reliability diagram for quantile 0.05 from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89371c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_diagram(\n",
    "    cv_predictions_bqr_95, kind=\"quantile\", quantile_level=0.95\n",
    ").interactive().properties(\n",
    "    title=\"Reliability diagram for quantile 0.95 from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233c2c64",
   "metadata": {},
   "source": [
    "\n",
    "We can complement this assessment with the Lorenz curves, which only assess\n",
    "the ranking power of the predictions, irrespective of their absolute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de628063",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lorenz_curve(cv_predictions_bqr_50).interactive().properties(\n",
    "    title=\"Lorenz curve for quantile 0.50 from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05071164",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lorenz_curve(cv_predictions_bqr_05).interactive().properties(\n",
    "    title=\"Lorenz curve for quantile 0.05 from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f111d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lorenz_curve(cv_predictions_bqr_95).interactive().properties(\n",
    "    title=\"Lorenz curve for quantile 0.95 from cross-validation predictions\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
