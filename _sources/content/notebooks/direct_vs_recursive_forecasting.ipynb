{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fcf9b3",
   "metadata": {},
   "source": [
    "# Direct vs Recursive Forecasting\n",
    "\n",
    "The purpose of this notebook is to compare the performance of direct\n",
    "forecasting and recursive forecasting using the `MLForecast` library. Direct\n",
    "forecasting means that we train a family of model to predict the target value\n",
    "at various horizons in the future, e.g. 1 hour, 2 hours, ..., 24 hours ahead.\n",
    "Recursive forecasting (also known as auto-regressive forecasting) means that\n",
    "we train a single model to predict the target value at the next time step,\n",
    "and then use the model recursively to predict the next time step using the\n",
    "previous predictions as input features. Implementing recursive forecasting is\n",
    "a bit cumbersome to do manually, hence we use the `MLForecast` library to\n",
    "handle this for us.\n",
    "\n",
    "The objective is to show that recursive forecasting can be more efficient in\n",
    "terms of memory usage and training time. However, it can also lead to a loss\n",
    "of accuracy because recursive calls are fed with previous predictions that do\n",
    "not necessarily match the training distribution of the model, and can\n",
    "therefore lead to degenerate predictions, in particular when the variance of\n",
    "the lagged values is informative.\n",
    "\n",
    "We highlight this issue with a synthetic dataset that has two types of\n",
    "segments:\n",
    "\n",
    "- Segment type \"a\" has a prefix centered around 0 with low variance and a\n",
    "  suffix centered around 1.\n",
    "- Segment type \"b\" has a prefix centered around 0 with high variance and a\n",
    "  suffix centered around -1.\n",
    "\n",
    "Segment of type \"a\" and \"b\" are independently sampled, meaning that is not\n",
    "possible to forecast beyond the length of the segments. However, it should be\n",
    "quite easy to predict the end of a segment given the prefix of the segment\n",
    "with lagged feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9f3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "SEGMENT_LENGTH = 30\n",
    "\n",
    "\n",
    "def generate_synthetic_1(\n",
    "    segment_length=SEGMENT_LENGTH,\n",
    "    n_segments=100,\n",
    "    low_noise_level=0.01,\n",
    "    high_noise_level=0.1,\n",
    "    seed=None,\n",
    "):\n",
    "    \"\"\"Generate synthetic time series data with two types of segments\n",
    "\n",
    "    - segment type \"a\" has a prefix centered around 0 and a suffix centered\n",
    "      around 1.\n",
    "    - segment type \"b\" has a prefix centered around 0 with high variance and a\n",
    "      suffix centered around -1.\n",
    "\n",
    "    The variance of the prefix is therefore predictive of the suffix.\n",
    "\n",
    "    The suffix values predictive of the next segment prefix's mean (always 0).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    total_length = segment_length * n_segments\n",
    "    segment_types = rng.choice([\"a\", \"b\"], n_segments)\n",
    "    prefix_length = segment_length // 2\n",
    "    suffix_length = segment_length - prefix_length\n",
    "\n",
    "    segments = []\n",
    "    for segment_type in segment_types:\n",
    "        if segment_type == \"a\":\n",
    "            # Prefix is centered around 0 with low variance\n",
    "            segments.append(\n",
    "                rng.normal(loc=0, scale=low_noise_level, size=prefix_length)\n",
    "            )\n",
    "            # Suffix is centered around 1 with low variance\n",
    "            segments.append(\n",
    "                rng.normal(loc=1, scale=low_noise_level, size=suffix_length)\n",
    "            )\n",
    "        elif segment_type == \"b\":\n",
    "            # Prefix is also centered around 0 but with high variance\n",
    "            segments.append(\n",
    "                rng.normal(loc=0, scale=high_noise_level, size=prefix_length)\n",
    "            )\n",
    "            # Suffix is centered around -1 with low variance\n",
    "            segments.append(\n",
    "                rng.normal(loc=-1, scale=low_noise_level, size=suffix_length)\n",
    "            )\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"time\": np.arange(total_length),\n",
    "            \"y\": np.concatenate(segments),\n",
    "            \"series_id\": np.zeros(total_length, dtype=np.int32),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "data = generate_synthetic_1(n_segments=500, seed=1)\n",
    "cutoff = -SEGMENT_LENGTH * 10  # 10 segments for testing\n",
    "data_train = data.iloc[:cutoff]\n",
    "data_test = data.iloc[cutoff:]\n",
    "_ = data_train.plot(x=\"time\", y=\"y\", title=\"Synthetic data 1\", figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca43e5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = data_train.iloc[: SEGMENT_LENGTH * 10].plot(\n",
    "    x=\"time\",\n",
    "    y=\"y\",\n",
    "    title=\"Synthetic data 1 - First points of training set\",\n",
    "    figsize=(15, 5),\n",
    ")\n",
    "\n",
    "_ = data_test.iloc[: SEGMENT_LENGTH * 10].plot(\n",
    "    x=\"time\",\n",
    "    y=\"y\",\n",
    "    title=\"Synthetic data 1 - First points of testing set\",\n",
    "    figsize=(15, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea296f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlforecast import MLForecast\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import SplineTransformer, PolynomialFeatures\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from mlforecast.lag_transforms import (\n",
    "    RollingMax,\n",
    "    RollingMin,\n",
    "    RollingMean,\n",
    "    RollingStd,\n",
    ")\n",
    "from mlforecast.target_transforms import Differences\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import warnings\n",
    "import threadpoolctl\n",
    "\n",
    "# Workaround a performance problem with HistGradientBoostingRegressor on small datasets.\n",
    "threadpoolctl.threadpool_limits(limits=1, user_api=\"openmp\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"sklearn\")\n",
    "\n",
    "\n",
    "# MLForecast can train multiple models in parallel, each model can be a\n",
    "# pipeline of transformers and a regressor. However we focus on a single\n",
    "# HistGradientBoostingRegressor model to make sure that this notebook runs\n",
    "# quickly enough. Feel free to uncomment the other models to compare their\n",
    "# performance if you have enough time and memory available.\n",
    "#\n",
    "# Spoiler alert: the HistGradientBoostingRegressor model is the most accurate.\n",
    "\n",
    "mlf = MLForecast(\n",
    "    models=[\n",
    "        # make_pipeline(\n",
    "        #     SplineTransformer(sparse_output=True, n_knots=10),\n",
    "        #     PolynomialFeatures(degree=2, include_bias=False, interaction_only=True),\n",
    "        #     # Nystroem(kernel=\"poly\", n_components=200, degree=2, random_state=0),\n",
    "        #     SelectKBest(k=100),\n",
    "        #     Ridge(alpha=1e-6),\n",
    "        # ),\n",
    "        # RandomForestRegressor(\n",
    "        #     n_estimators=100,\n",
    "        #     max_features=0.8,\n",
    "        #     max_depth=8,\n",
    "        #     min_samples_leaf=300,\n",
    "        #     n_jobs=4,\n",
    "        # ),\n",
    "        # DecisionTreeRegressor(max_depth=8, min_samples_leaf=300),\n",
    "        HistGradientBoostingRegressor(),\n",
    "    ],\n",
    "    freq=1,\n",
    "    lags=range(1, SEGMENT_LENGTH + 1),\n",
    "    lag_transforms={\n",
    "        1: [\n",
    "            RollingMean(SEGMENT_LENGTH // 2),\n",
    "            RollingStd(SEGMENT_LENGTH // 2),\n",
    "        ],\n",
    "        SEGMENT_LENGTH\n",
    "        // 2: [\n",
    "            RollingMax(SEGMENT_LENGTH // 2),\n",
    "            RollingMin(SEGMENT_LENGTH // 2),\n",
    "        ],\n",
    "    },\n",
    "    # target_transforms=[Differences([1])],\n",
    "    num_threads=4,\n",
    ")\n",
    "schema = dict(\n",
    "    time_col=\"time\",\n",
    "    id_col=\"series_id\",\n",
    "    target_col=\"y\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae37ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlf.preprocess(data_train, **schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cb55ac",
   "metadata": {},
   "source": [
    "\n",
    "## Recursive or auto-regressive forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0edc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = perf_counter()\n",
    "mlf.fit(data_train, **schema)  # recursive forecasting by default in mlforecast\n",
    "print(f\"Recursive forecasting training time: {perf_counter() - tic:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec638e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_HORIZON = SEGMENT_LENGTH * 2\n",
    "\n",
    "\n",
    "def collect_predictions(mlf, data_test, test_offset=0):\n",
    "    \"\"\"Collect predictions from the MLForecast object.\"\"\"\n",
    "    all_predictions = []\n",
    "    UPDATE_CHUNK_SIZE = 5\n",
    "    while test_offset < len(data_test):\n",
    "\n",
    "        new_predictions = mlf.predict(PREDICTION_HORIZON)\n",
    "        new_predictions[\"horizon\"] = np.arange(new_predictions.shape[0]) + 1\n",
    "        new_predictions = new_predictions.merge(\n",
    "            data_test, on=[\"time\", \"series_id\"], how=\"left\"\n",
    "        )\n",
    "        all_predictions.append(new_predictions)\n",
    "\n",
    "        # Update the forecaster with the new observations\n",
    "        mlf.update(data_test.iloc[test_offset : test_offset + UPDATE_CHUNK_SIZE])\n",
    "        test_offset += UPDATE_CHUNK_SIZE\n",
    "\n",
    "    return all_predictions\n",
    "\n",
    "\n",
    "tic = perf_counter()\n",
    "all_recursive_predictions = collect_predictions(mlf, data_test)\n",
    "print(f\"Recursive forecasting prediction time: {perf_counter() - tic:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1ff02a",
   "metadata": {},
   "source": [
    "\n",
    "## Direct forecasting\n",
    "\n",
    "Let's pass `max_horizon` to force modeling for direct forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b39c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = perf_counter()\n",
    "mlf.fit(data_train, max_horizon=PREDICTION_HORIZON, **schema)\n",
    "print(f\"Direct forecasting training time: {perf_counter() - tic:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c03bcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = perf_counter()\n",
    "all_direct_predictions = collect_predictions(mlf, data_test)\n",
    "print(f\"Direct forecasting prediction time: {perf_counter() - tic:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b47998",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "## Quantitative comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea64d535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_predictions(all_predictions, model_name):\n",
    "    \"\"\"Compute the mean absolute error of the predictions.\"\"\"\n",
    "    all_predictions = pd.concat(all_predictions)\n",
    "    all_predictions[\"absolute_error\"] = np.abs(\n",
    "        all_predictions[\"y\"] - all_predictions[model_name]\n",
    "    )\n",
    "    return all_predictions.dropna().groupby(\"horizon\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "score_predictions(\n",
    "    all_recursive_predictions, \"HistGradientBoostingRegressor\"\n",
    ").mean().reset_index().plot(x=\"horizon\", y=\"absolute_error\", label=\"recursive\", ax=ax)\n",
    "score_predictions(\n",
    "    all_direct_predictions, \"HistGradientBoostingRegressor\"\n",
    ").mean().reset_index().plot(x=\"horizon\", y=\"absolute_error\", label=\"direct \", ax=ax)\n",
    "_ = ax.set(ylabel=\"MAE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e704bd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "## Qualitative comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dba3b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_some_predictions(all_predictions, data_test, model_name, nrows=12, title=None):\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=nrows, figsize=(15, 5 * nrows))\n",
    "    for row_idx, predictions in enumerate(all_predictions):\n",
    "        predictions = predictions.drop(\"y\", axis=1)\n",
    "        merged_data = data_test.copy()\n",
    "        merged_data = merged_data.merge(\n",
    "            predictions, on=[\"time\", \"series_id\"], how=\"left\"\n",
    "        )\n",
    "        merged_data.drop([\"series_id\"], axis=1).iloc[: SEGMENT_LENGTH * 3].plot(\n",
    "            x=\"time\", y=[\"y\", model_name], ax=axes[row_idx]\n",
    "        )\n",
    "        axes[row_idx].set_title(title)\n",
    "        axes[row_idx].set_ylim(-1.2, 1.2)\n",
    "\n",
    "        if row_idx >= nrows - 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5f9a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_some_predictions(\n",
    "    all_recursive_predictions,\n",
    "    data_test,\n",
    "    \"HistGradientBoostingRegressor\",\n",
    "    title=\"recursive\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf7900",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_some_predictions(\n",
    "    all_direct_predictions, data_test, \"HistGradientBoostingRegressor\", title=\"direct\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
