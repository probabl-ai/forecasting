{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59b57ed8",
   "metadata": {},
   "source": [
    "# Direct vs Recursive Forecasting\n",
    "\n",
    "The purpose of this notebook is to compare the performance of direct\n",
    "forecasting and recursive forecasting using the `MLForecast` library. Direct\n",
    "forecasting means that we train a family of model to predict the target value\n",
    "at various horizons in the future, e.g. 1 hour, 2 hours, ..., 24 hours ahead.\n",
    "Recursive forecasting (also known as auto-regressive forecasting) means that\n",
    "we train a single model to predict the target value at the next time step,\n",
    "and then use the model recursively to predict the next time step using the\n",
    "previous predictions as input features. Implementing recursive forecasting is\n",
    "a bit cumbersome to do manually, hence we use the `MLForecast` library to\n",
    "handle this for us.\n",
    "\n",
    "The objective is to show that recursive forecasting can be more efficient in\n",
    "terms of memory usage and training time. However, it can also lead to a loss\n",
    "of accuracy because recursive calls are fed with previous predictions that do\n",
    "not necessarily match the training distribution of the model, and can\n",
    "therefore lead to degenerate predictions, in particular when the variance of\n",
    "the lagged values is informative.\n",
    "\n",
    "We highlight this issue with a synthetic dataset that has two types of\n",
    "segments:\n",
    "\n",
    "- Segment type \"a\" has a prefix centered around 0 with low variance and a\n",
    "  suffix centered around 1.\n",
    "- Segment type \"b\" has a prefix centered around 0 with high variance and a\n",
    "  suffix centered around -1.\n",
    "\n",
    "Segment of type \"a\" and \"b\" are independently sampled, meaning that is not\n",
    "possible to forecast beyond the length of the segments. However, it should be\n",
    "quite easy to predict the end of a segment given the prefix of the segment\n",
    "with lagged feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140eb7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "SEGMENT_LENGTH = 30\n",
    "\n",
    "\n",
    "def generate_synthetic_1(\n",
    "    segment_length=SEGMENT_LENGTH,\n",
    "    n_segments=100,\n",
    "    low_noise_level=0.01,\n",
    "    high_noise_level=0.1,\n",
    "    seed=None,\n",
    "):\n",
    "    \"\"\"Generate synthetic time series data with two types of segments\n",
    "\n",
    "    - segment type \"a\" has a prefix centered around 0 and a suffix centered\n",
    "      around 1.\n",
    "    - segment type \"b\" has a prefix centered around 0 with high variance and a\n",
    "      suffix centered around -1.\n",
    "\n",
    "    The variance of the prefix is therefore predictive of the suffix.\n",
    "\n",
    "    The suffix values predictive of the next segment prefix's mean (always 0).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    total_length = segment_length * n_segments\n",
    "    segment_types = rng.choice([\"a\", \"b\"], n_segments)\n",
    "    prefix_length = segment_length // 2\n",
    "    suffix_length = segment_length - prefix_length\n",
    "\n",
    "    segments = []\n",
    "    for segment_type in segment_types:\n",
    "        if segment_type == \"a\":\n",
    "            # Prefix is centered around 0 with low variance\n",
    "            segments.append(\n",
    "                rng.normal(loc=0, scale=low_noise_level, size=prefix_length)\n",
    "            )\n",
    "            # Suffix is centered around 1 with low variance\n",
    "            segments.append(\n",
    "                rng.normal(loc=1, scale=low_noise_level, size=suffix_length)\n",
    "            )\n",
    "        elif segment_type == \"b\":\n",
    "            # Prefix is also centered around 0 but with high variance\n",
    "            segments.append(\n",
    "                rng.normal(loc=0, scale=high_noise_level, size=prefix_length)\n",
    "            )\n",
    "            # Suffix is centered around -1 with low variance\n",
    "            segments.append(\n",
    "                rng.normal(loc=-1, scale=low_noise_level, size=suffix_length)\n",
    "            )\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"time\": np.arange(total_length),\n",
    "            \"y\": np.concatenate(segments),\n",
    "            \"series_id\": np.zeros(total_length, dtype=np.int32),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "data = generate_synthetic_1(n_segments=500, seed=1)\n",
    "cutoff = -SEGMENT_LENGTH * 10  # 10 segments for testing\n",
    "data_train = data.iloc[:cutoff]\n",
    "data_test = data.iloc[cutoff:]\n",
    "_ = data_train.plot(x=\"time\", y=\"y\", title=\"Synthetic data 1\", figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaf24a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = data_train.iloc[: SEGMENT_LENGTH * 10].plot(\n",
    "    x=\"time\",\n",
    "    y=\"y\",\n",
    "    title=\"Synthetic data 1 - First points of training set\",\n",
    "    figsize=(15, 5),\n",
    ")\n",
    "\n",
    "_ = data_test.iloc[: SEGMENT_LENGTH * 10].plot(\n",
    "    x=\"time\",\n",
    "    y=\"y\",\n",
    "    title=\"Synthetic data 1 - First points of testing set\",\n",
    "    figsize=(15, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199618ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlforecast import MLForecast\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import SplineTransformer, PolynomialFeatures\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from mlforecast.lag_transforms import (\n",
    "    Combine,\n",
    "    RollingMax,\n",
    "    RollingMin,\n",
    "    RollingMean,\n",
    "    RollingStd,\n",
    ")\n",
    "from mlforecast.target_transforms import Differences\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import warnings\n",
    "import threadpoolctl\n",
    "\n",
    "# Workaround a performance problem with HistGradientBoostingRegressor on small datasets.\n",
    "threadpoolctl.threadpool_limits(limits=1, user_api=\"openmp\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"sklearn\")\n",
    "\n",
    "\n",
    "mlf = MLForecast(\n",
    "    models=[\n",
    "        # Ridge(alpha=1e-6),\n",
    "        # make_pipeline(SplineTransformer(), Ridge(alpha=1e-6)),\n",
    "        # make_pipeline(\n",
    "        #     SplineTransformer(sparse_output=True, n_knots=10),\n",
    "        #     PolynomialFeatures(degree=2, include_bias=False, interaction_only=True),\n",
    "        #     # Nystroem(kernel=\"poly\", n_components=200, degree=2, random_state=0),\n",
    "        #     SelectKBest(k=100),\n",
    "        #     Ridge(alpha=1e-6),\n",
    "        # ),\n",
    "        # RandomForestRegressor(\n",
    "        #     n_estimators=100,\n",
    "        #     max_features=0.8,\n",
    "        #     max_depth=8,\n",
    "        #     min_samples_leaf=300,\n",
    "        #     n_jobs=4,\n",
    "        # ),\n",
    "        # DecisionTreeRegressor(max_depth=8, min_samples_leaf=300),\n",
    "        HistGradientBoostingRegressor(),\n",
    "    ],\n",
    "    freq=1,\n",
    "    lags=range(1, SEGMENT_LENGTH + 1),\n",
    "    lag_transforms={\n",
    "        1: [\n",
    "            RollingMean(SEGMENT_LENGTH // 2),\n",
    "            RollingStd(SEGMENT_LENGTH // 2),\n",
    "        ],\n",
    "        SEGMENT_LENGTH\n",
    "        // 2: [\n",
    "            RollingMax(SEGMENT_LENGTH // 2),\n",
    "            RollingMin(SEGMENT_LENGTH // 2),\n",
    "        ],\n",
    "    },\n",
    "    # target_transforms=[Differences([1])],\n",
    "    num_threads=4,\n",
    ")\n",
    "schema = dict(\n",
    "    time_col=\"time\",\n",
    "    id_col=\"series_id\",\n",
    "    target_col=\"y\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20351c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlf.preprocess(data_train, **schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c3233b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "PREDICTION_HORIZON = SEGMENT_LENGTH * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2496148",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# mlf.fit(data_train, max_horizon=PREDICTION_HORIZON, **schema) # direct forecasting\n",
    "mlf.fit(data_train, **schema)  # recursive forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9867dc3b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "test_offset = 0\n",
    "\n",
    "all_predictions = []\n",
    "UPDATE_CHUNK_SIZE = 5\n",
    "while test_offset < len(data_test):\n",
    "\n",
    "    new_predictions = mlf.predict(PREDICTION_HORIZON)\n",
    "    all_predictions.append(new_predictions)\n",
    "\n",
    "    # Update the forecaster with the new observations\n",
    "    mlf.update(data_test.iloc[test_offset : test_offset + UPDATE_CHUNK_SIZE])\n",
    "    test_offset += UPDATE_CHUNK_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09157d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "nrows = 12\n",
    "fig, axes = plt.subplots(nrows=nrows, figsize=(15, 5 * nrows))\n",
    "for row_idx, predictions in enumerate(all_predictions):\n",
    "    merged_data = data_test.copy()\n",
    "    merged_data = merged_data.merge(predictions, on=[\"time\", \"series_id\"], how=\"left\")\n",
    "\n",
    "    merged_data.drop([\"series_id\"], axis=1).iloc[: SEGMENT_LENGTH * 3].plot(\n",
    "        x=\"time\", ax=axes[row_idx]\n",
    "    )\n",
    "    axes[row_idx].set_ylim(-1.2, 1.2)\n",
    "\n",
    "    if row_idx >= nrows - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e87ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
